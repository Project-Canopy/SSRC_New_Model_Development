{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from boto3 import client\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "j_file = \"train_test_polygons.json\"\n",
    "\n",
    "with open(j_file, 'r') as j:\n",
    "    train_test = json.loads(j.read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Chip List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('yes_chips_s3.json', 'w') as fp:\n",
    "    json.dump(chips, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chips/cloudfree-merge-polygons/yes/Habitation/100/101/101_1000_3000.tif',\n",
       " 'chips/cloudfree-merge-polygons/yes/Habitation/100/101/101_1000_3100.tif',\n",
       " 'chips/cloudfree-merge-polygons/yes/Habitation/100/101/101_1000_400.tif']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chips[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train / Test Copy Based on JSON Train / Test Chip List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_s3_copy(chips, j_file, bucket_name='canopy-production-ml',\n",
    "                       base_path='chips/cloudfree-merge-polygons/split/'):\n",
    "    \n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "    \n",
    "    with open(j_file, 'r') as j:\n",
    "        \n",
    "        train_test_file = json.loads(j.read())\n",
    "        \n",
    "#     for split in list(train_test.keys()):\n",
    "        \n",
    "#         for polygon in train_test[split]:\n",
    "\n",
    "    length = len(chips)\n",
    "            \n",
    "    for i, chip in enumerate(chips, 1):\n",
    "        print(f'Processing chip {i} of {length}', end='\\r', flush=True)\n",
    "\n",
    "        CopySource = {\n",
    "            'Bucket': bucket_name,\n",
    "            'Key': chip\n",
    "        }\n",
    "\n",
    "        polygon_id = int(chip.split(\"/\")[5])\n",
    "\n",
    "        filename = chip.split('/')[-1]\n",
    "\n",
    "        if polygon_id in train_test_file[\"test\"]:\n",
    "            train_test = 'test'\n",
    "\n",
    "            #s3.Object(bucket, base_path + 'test/').copy_from(CopySource=bucket + chip.key)\n",
    "\n",
    "        else:\n",
    "            train_test = 'train'\n",
    "\n",
    "            #s3.Object(bucket, base_path + 'train_val/').copy_from(CopySource=bucket + chip.key)\n",
    "\n",
    "        new_key = f'{base_path}{train_test}/{polygon_id}/{filename}'\n",
    "\n",
    "        bucket.copy(CopySource, new_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chip 49133 of 49133\r"
     ]
    }
   ],
   "source": [
    "j_file = \"train_test_polygons.json\"\n",
    "train_test_s3_copy(chips,j_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train / Val Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3 Master Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s3_actions(bucket, in_path=None, out_path=None, copy_list=None, copy_dir=False, delete=False, stop_at=None, chip_list=False, chip_count=False):\n",
    "    s3 = boto3.resource('s3')\n",
    "    my_bucket = s3.Bucket(bucket)\n",
    "    objs = []\n",
    "    total_files = 0 \n",
    "    \n",
    "\n",
    "    \n",
    "    if copy_dir or delete or chip_list or chip_count:\n",
    "        for obj in my_bucket.objects.filter(Prefix=in_path):\n",
    "            total_files += 1 \n",
    "                objs.append(obj)\n",
    "            if total_files == stop_at:\n",
    "                break\n",
    "        \n",
    "    tot_objs = len(objs)\n",
    "        \n",
    "    if delete:\n",
    "        for index,obj in enumerate(objs,1):\n",
    "            print(f\"deleting {index} of {tot_objs}\", end='\\r', flush=True)\n",
    "            obj.delete()\n",
    "            \n",
    "    if chip_list:\n",
    "        return objs\n",
    "    \n",
    "    if chip_count:\n",
    "        return total_files\n",
    "    \n",
    "    if copy_list:\n",
    "        total_copy_list = len(copy_list)\n",
    "\n",
    "        for index,obj in enumerate(copy_list,1):\n",
    "            \n",
    "            print(f\"copying {index} of {total_copy_list}\", end='\\r', flush=True)\n",
    "            old_key = in_path + obj\n",
    "            new_key = out_path + obj\n",
    "            CopySource = {\n",
    "                'Bucket': bucket,\n",
    "                'Key':old_key}\n",
    "            my_bucket.copy(CopySource, new_key)\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'canopy-production-ml'\n",
    "paths = ['chips/cloudfree-merge-polygons/split/train_copy/',\n",
    "         'chips/cloudfree-merge-polygons/split/train/',\n",
    "        'chips/cloudfree-merge-polygons/split/test/',\n",
    "        'chips/cloudfree-merge-polygons/yes/',\n",
    "        'chips/cloudfree-merge-polygons/split/val/']\n",
    "    \n",
    "dirs = [s3_actions(bucket,i,chip_list=True) for i in paths[:2]]\n",
    "\n",
    "# s3_actions(bucket,paths,chip_count=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34071"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_chips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '1_1200_1800.tif']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirs[0][1].key.split(\"/\")[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_t_copy = []\n",
    "filenames_t = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = [filenames_t.append(\"/\".join(i.key.split(\"/\")[-2:])) for i in dirs[1]]\n",
    "f = [filenames_t_copy.append(\"/\".join(i.key.split(\"/\")[-2:])) for i in dirs[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_chips = list(set(filenames_t_copy) ^ set(filenames_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_chips.remove(\"/\")\n",
    "new_chips.remove(\"train/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1/1_1400_1100.tif',\n",
       " '1/1_1200_900.tif',\n",
       " '10/10_1700_3400.tif',\n",
       " '10/10_1700_2300.tif',\n",
       " '1/1_1300_2000.tif',\n",
       " '10/10_1700_3600.tif',\n",
       " '1/1_1500_1100.tif',\n",
       " '1/1_1400_900.tif',\n",
       " '10/10_1700_2000.tif',\n",
       " '1/1_1300_1600.tif',\n",
       " '1/1_1400_1700.tif',\n",
       " '10/10_1800_2400.tif',\n",
       " '10/10_1700_2900.tif',\n",
       " '1/1_1300_1400.tif',\n",
       " '1/1_1500_1400.tif',\n",
       " '1/1_1400_1000.tif',\n",
       " '10/10_1800_2500.tif',\n",
       " '1/1_1500_1500.tif',\n",
       " '1/1_1200_1900.tif',\n",
       " '10/10_1700_2500.tif',\n",
       " '1/1_1500_1200.tif',\n",
       " '1/1_1400_1600.tif',\n",
       " '1/1_1300_800.tif',\n",
       " '10/10_1700_2600.tif',\n",
       " '1/1_1300_1100.tif',\n",
       " '1/1_1400_700.tif',\n",
       " '10/10_1700_3200.tif',\n",
       " '1/1_1400_1300.tif',\n",
       " '1/1_1400_1500.tif',\n",
       " '1/1_1400_600.tif',\n",
       " '10/10_1800_2600.tif',\n",
       " '1/1_1400_1800.tif',\n",
       " '1/1_1300_700.tif',\n",
       " '1/1_1300_900.tif',\n",
       " '1/1_1200_2100.tif',\n",
       " '10/10_1600_3900.tif',\n",
       " '10/10_1700_3500.tif',\n",
       " '1/1_1400_2000.tif',\n",
       " '10/10_1800_2000.tif',\n",
       " '1/1_1200_1800.tif',\n",
       " '10/10_1700_2800.tif',\n",
       " '1/1_1300_1800.tif',\n",
       " '10/10_1700_2100.tif',\n",
       " '1/1_1200_2000.tif',\n",
       " '1/1_1200_700.tif',\n",
       " '1/1_1300_600.tif',\n",
       " '10/10_1800_2300.tif',\n",
       " '1/1_1300_1700.tif',\n",
       " '1/1_1400_1400.tif',\n",
       " '10/10_1700_2400.tif',\n",
       " '1/1_1200_800.tif',\n",
       " '10/10_1700_3000.tif',\n",
       " '10/10_1700_3100.tif',\n",
       " '1/1_1300_1900.tif',\n",
       " '10/10_1800_2200.tif',\n",
       " '1/1_1400_800.tif',\n",
       " '10/10_1700_3300.tif',\n",
       " '10/10_1800_2100.tif',\n",
       " '1/1_1400_1200.tif',\n",
       " '1/1_1300_1500.tif',\n",
       " '1/1_1300_1000.tif',\n",
       " '1/1_1400_1900.tif',\n",
       " '10/10_1700_3800.tif',\n",
       " '1/1_1500_1000.tif',\n",
       " '1/1_1200_500.tif',\n",
       " '10/10_1700_1900.tif',\n",
       " '1/1_1200_600.tif',\n",
       " '1/1_1500_1300.tif',\n",
       " '10/10_1700_2700.tif',\n",
       " '1/1_1300_1200.tif',\n",
       " '10/10_1700_2200.tif',\n",
       " '10/10_1700_3700.tif',\n",
       " '1/1_1300_1300.tif',\n",
       " '10/10_1700_3900.tif']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_chips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copying 74 of 74\r"
     ]
    }
   ],
   "source": [
    "s3_actions(bucket,in_path=\"chips/cloudfree-merge-polygons/split/train_copy/\", \n",
    "           out_path=\"chips/cloudfree-merge-polygons/split/train/\",copy_list=new_chips)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train / Val Split - Copy Train Chips to Val / Delete Same Chips from Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(train_chips,bucket_name='canopy-production-ml',\n",
    "                    in_base_path='chips/cloudfree-merge-polygons/split/',\n",
    "                    out_base_path='chips/cloudfree-merge-polygons/split/val/',\n",
    "                   copy=True,\n",
    "                   delete=True):\n",
    "    \n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "    train_chips = train_chips.copy()\n",
    "    dir_dict = {}\n",
    "    \n",
    "    for chip_1 in train_chips:\n",
    "        \n",
    "        chip_key = chip_1.key\n",
    "        poly_id = chip_key.split(\"/\")[-1].split(\"_\")[0]\n",
    "        \n",
    "        if poly_id in dir_dict.keys():\n",
    "                \n",
    "            dir_dict[poly_id].append(chip_key)\n",
    "                \n",
    "        else:\n",
    "\n",
    "            dir_dict[poly_id] = [chip_key]\n",
    "            \n",
    "    \n",
    "    key_num = len(dir_dict.keys())\n",
    "    \n",
    "    for i_1,key in enumerate(dir_dict.keys(),1):\n",
    "        \n",
    "        dir_len = len(dir_dict[key])\n",
    "        val_slice = dir_len // 5 \n",
    "        val_chips = dir_dict[key][:val_slice]\n",
    "        val_num = len(val_chips)\n",
    "        \n",
    "        for i_2,val_chip_key in enumerate(val_chips,1):\n",
    "            \n",
    "            print(f\"Copying chip {i_2} of {val_num} in polygon {i_1} of {key_num}\", end='\\r', flush=True)\n",
    "            polygon_id = val_chip_key.split(\"/\")[-1].split(\"_\")[0]\n",
    "            filename = val_chip_key.split('/')[-1]\n",
    "            new_key = f'{out_base_path}{polygon_id}/{filename}'\n",
    "            \n",
    "            if copy:\n",
    "                \n",
    "                CopySource = {\n",
    "                'Bucket': bucket_name,\n",
    "                'Key':val_chip_key}\n",
    "                \n",
    "                bucket.copy(CopySource, new_key)\n",
    "            \n",
    "            if delete:\n",
    "            \n",
    "                obj = s3.Object(bucket_name, val_chip_key)\n",
    "                obj.delete()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying chip 285 of 285 in polygon 47 of 4747\r"
     ]
    }
   ],
   "source": [
    "train_val_split(train_chips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_chips = s3_list(dir_name='val',bucket='canopy-production-ml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[s3.ObjectSummary(bucket_name='canopy-production-ml', key='chips/cloudfree-merge-polygons/split/train/'),\n",
       " s3.ObjectSummary(bucket_name='canopy-production-ml', key='chips/cloudfree-merge-polygons/split/train/1/1_1000_1000.tif'),\n",
       " s3.ObjectSummary(bucket_name='canopy-production-ml', key='chips/cloudfree-merge-polygons/split/train/1/1_1000_1100.tif')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_chips[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_dir(file_list,\n",
    "             out_base_path='chips/cloudfree-merge-polygons/split/train_copy/',\n",
    "             bucket_name='canopy-production-ml'):\n",
    "\n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "    dir_num = len(file_list)\n",
    "\n",
    "    for i_2,chip in enumerate(file_list,1):\n",
    "\n",
    "        chip_key = chip.key\n",
    "\n",
    "        print(f\"Copying chip {i_2} of {dir_num}\", end='\\r', flush=True)\n",
    "\n",
    "        CopySource = {\n",
    "            'Bucket': 'canopy-production-ml',\n",
    "            'Key':chip_key}\n",
    "\n",
    "        polygon_id = chip_key.split(\"/\")[-1].split(\"_\")[0]\n",
    "\n",
    "        filename = chip_key.split('/')[-1]\n",
    "\n",
    "        new_key = f'{out_base_path}{polygon_id}/{filename}'\n",
    "\n",
    "        bucket.copy(CopySource, new_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying chip 34442 of 34442\r"
     ]
    }
   ],
   "source": [
    "copy_dir(train_chips[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_chips' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-1fce4c5e8a68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mchip\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_chips\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'canopy-production-ml'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#     obj.delete()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'val_chips' is not defined"
     ]
    }
   ],
   "source": [
    "for chip in train[1:2]:\n",
    "    \n",
    "    obj = s3.Object('canopy-production-ml', chip.key)\n",
    "    \n",
    "#     obj.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s3_actions(bucket, path, copy=False, delete=False, stop_at=None, chip_list=False, chip_count=False):\n",
    "    s3 = boto3.resource('s3')\n",
    "    my_bucket = s3.Bucket(bucket)\n",
    "    objs = []\n",
    "    total_files = 0 \n",
    "\n",
    "    for obj in my_bucket.objects.filter(Prefix=path):\n",
    "        \n",
    "        total_files += 1 \n",
    "        \n",
    "        if copy or delete:\n",
    "        \n",
    "            objs.append(obj)\n",
    "\n",
    "        if total_files == stop_at:\n",
    "\n",
    "            break\n",
    "        \n",
    "    tot_objs = len(objs)\n",
    "        \n",
    "    if delete:\n",
    "\n",
    "        for index,obj in enumerate(objs,1):\n",
    "            \n",
    "            print(f\"deleting {index} of {tot_objs}\", end='\\r', flush=True)\n",
    "            \n",
    "            obj.delete()\n",
    "            \n",
    "    if chip_list:\n",
    "        \n",
    "        return objs\n",
    "    \n",
    "    if chip_count:\n",
    "        \n",
    "        return total_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6795"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket = 'canopy-production-ml'\n",
    "paths = ['chips/cloudfree-merge-polygons/split/train_copy/',\n",
    "        'chips/cloudfree-merge-polygons/split/test/',\n",
    "        'chips/cloudfree-merge-polygons/yes/',\n",
    "        'chips/cloudfree-merge-polygons/split/val/']\n",
    "    \n",
    "# dirs = [s3_actions(bucket,i,chip_count=True) for i in paths]\n",
    "\n",
    "s3_actions(bucket,paths[3],chip_count=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[34071, 7707, 49133]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41778"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirs[0] + dirs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "labelling-2",
   "language": "python",
   "name": "labelling-2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
