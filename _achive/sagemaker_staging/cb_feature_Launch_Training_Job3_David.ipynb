{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook Instance Imports\n",
    "import os\n",
    "import sagemaker\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.debugger import ProfilerConfig, FrameworkProfile\n",
    "import time\n",
    "import io\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "profiler_config=ProfilerConfig(\n",
    "    framework_profile_params=FrameworkProfile()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Data Location for Docker Local Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': 's3://canopy-production-ml/training_inputs/train_val_test/'}\n",
      "{'wandb_key': 'ded96d05c0cfafc1f209276af6c21cb7ac61e5de', 'epochs': '10', 's3_chkpt_dir': 'ckpt', 'augment': 'False', 'flip_left_right': 'False', 'flip_up_down': 'False', 'rot90': 'True'}\n"
     ]
    }
   ],
   "source": [
    "# S3 directories\n",
    "\n",
    "training_files = \"s3://canopy-production-ml/training_inputs/train_val_test/\"\n",
    "# val_file = \"s3://canopy-production-ml/training_inputs/val_labels.csv\"\n",
    "# labels_file = \"s3://canopy-production-ml/training_inputs/labels.json\"\n",
    "\n",
    "inputs = {\"data\":training_files}\n",
    "# hyperparameters = {\"wandb_key\":\"abfa0dec9fc06fbfa6392496f40a22a8d47e58cf\",\n",
    "#                    \"epochs\":\"10\",\n",
    "#                    \"s3_chkpt_dir\":\"ckpt\",\n",
    "#                    \"starting_weights\",\n",
    "#                    \"starting_epoch\",\n",
    "#                    \"batch_size\":\"20\",\n",
    "#                    \"learning_rate\",\n",
    "#                    \"bands_all\",\n",
    "#                    \"band_list\"\n",
    "#                    \"flip_left_right\",\n",
    "#                    \"flip_up_down\",\n",
    "#                    \"rot90\",\n",
    "#                    \"enable_shuffle\",\n",
    "#                   \"patience\"}\n",
    "\n",
    "hyperparameters = {\n",
    "    \"wandb_key\": \"ded96d05c0cfafc1f209276af6c21cb7ac61e5de\",\n",
    "    \"epochs\": \"10\",\n",
    "    \"s3_chkpt_dir\": \"ckpt\",\n",
    "    \"augment\": \"False\",\n",
    "    \"flip_left_right\": \"False\",\n",
    "    \"flip_up_down\": \"False\",\n",
    "    \"rot90\": \"True\"\n",
    "    #\"bands\": \"2,3,4,8,12\",\n",
    "    #\"starting_checkpoint\": \"ckpt/tf-custom-container-test-2021-03-15-18-18-39-877/last_chkpt.h5\"\n",
    "}\n",
    "\n",
    "print(inputs)\n",
    "print(hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Docker for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/cb_feature_detection/sagemaker_staging\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/cb_feature_detection/sagemaker_staging/docker_test_folder\n"
     ]
    }
   ],
   "source": [
    "%cd docker_test_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n"
     ]
    }
   ],
   "source": [
    "! aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin 763104351884.dkr.ecr.us-east-1.amazonaws.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  153.1kB\n",
      "Step 1/6 : FROM 763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-training:2.4.1-gpu-py37-cu110-ubuntu18.04\n",
      " ---> 993791d9475c\n",
      "Step 2/6 : ENV PATH=\"/opt/ml/code:${PATH}\"\n",
      " ---> Using cache\n",
      " ---> 24f6ef66670c\n",
      "Step 3/6 : RUN pip3 install rasterio wandb tensorflow-addons\n",
      " ---> Using cache\n",
      " ---> 2e0e92ca4149\n",
      "Step 4/6 : COPY cb_feature_train3_aws.py /opt/ml/code/train.py\n",
      " ---> 0c802efcf55c\n",
      "Step 5/6 : COPY data_loader.py /opt/ml/code/data_loader.py\n",
      " ---> 70ec557a634b\n",
      "Step 6/6 : ENV SAGEMAKER_PROGRAM train.py\n",
      " ---> Running in 1f6466b69e5f\n",
      "Removing intermediate container 1f6466b69e5f\n",
      " ---> b65aacad47d5\n",
      "Successfully built b65aacad47d5\n",
      "Successfully tagged tf-custom-container-test:latest\n"
     ]
    }
   ],
   "source": [
    "! docker build -t tf-custom-container-test ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Local Container test - Success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ikx8w1hx57-algo-1-39mtm ... \n",
      "Creating ikx8w1hx57-algo-1-39mtm ... done\n",
      "Attaching to ikx8w1hx57-algo-1-39mtm\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m 2021-03-17 22:57:06.526650: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m 2021-03-17 22:57:06.526867: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m 2021-03-17 22:57:06.531551: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m 2021-03-17 22:57:06.569714: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m 2021-03-17 22:57:08,545 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m 2021-03-17 22:57:08,546 sagemaker-training-toolkit INFO     Failed to parse hyperparameter wandb_key value ded96d05c0cfafc1f209276af6c21cb7ac61e5de to Json.\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m Returning the value itself\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m 2021-03-17 22:57:08,546 sagemaker-training-toolkit INFO     Failed to parse hyperparameter s3_chkpt_dir value ckpt to Json.\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m Returning the value itself\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m 2021-03-17 22:57:08,546 sagemaker-training-toolkit INFO     Failed to parse hyperparameter augment value False to Json.\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m Returning the value itself\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m 2021-03-17 22:57:08,546 sagemaker-training-toolkit INFO     Failed to parse hyperparameter flip_left_right value False to Json.\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m Returning the value itself\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m 2021-03-17 22:57:08,546 sagemaker-training-toolkit INFO     Failed to parse hyperparameter flip_up_down value False to Json.\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m Returning the value itself\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m 2021-03-17 22:57:08,546 sagemaker-training-toolkit INFO     Failed to parse hyperparameter rot90 value True to Json.\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m Returning the value itself\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m 2021-03-17 22:57:08,555 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m 2021-03-17 22:57:08,578 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m 2021-03-17 22:57:08,579 sagemaker-training-toolkit INFO     Failed to parse hyperparameter wandb_key value ded96d05c0cfafc1f209276af6c21cb7ac61e5de to Json.\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m Returning the value itself\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m 2021-03-17 22:57:08,579 sagemaker-training-toolkit INFO     Failed to parse hyperparameter s3_chkpt_dir value ckpt to Json.\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m Returning the value itself\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m 2021-03-17 22:57:08,579 sagemaker-training-toolkit INFO     Failed to parse hyperparameter augment value False to Json.\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m Returning the value itself\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m 2021-03-17 22:57:08,579 sagemaker-training-toolkit INFO     Failed to parse hyperparameter flip_left_right value False to Json.\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m Returning the value itself\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m 2021-03-17 22:57:08,579 sagemaker-training-toolkit INFO     Failed to parse hyperparameter flip_up_down value False to Json.\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m Returning the value itself\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m 2021-03-17 22:57:08,579 sagemaker-training-toolkit INFO     Failed to parse hyperparameter rot90 value True to Json.\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m Returning the value itself\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m 2021-03-17 22:57:08,603 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m 2021-03-17 22:57:08,604 sagemaker-training-toolkit INFO     Failed to parse hyperparameter wandb_key value ded96d05c0cfafc1f209276af6c21cb7ac61e5de to Json.\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m Returning the value itself\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m 2021-03-17 22:57:08,604 sagemaker-training-toolkit INFO     Failed to parse hyperparameter s3_chkpt_dir value ckpt to Json.\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m Returning the value itself\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m 2021-03-17 22:57:08,604 sagemaker-training-toolkit INFO     Failed to parse hyperparameter augment value False to Json.\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m Returning the value itself\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m 2021-03-17 22:57:08,604 sagemaker-training-toolkit INFO     Failed to parse hyperparameter flip_left_right value False to Json.\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m Returning the value itself\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m 2021-03-17 22:57:08,604 sagemaker-training-toolkit INFO     Failed to parse hyperparameter flip_up_down value False to Json.\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m Returning the value itself\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m 2021-03-17 22:57:08,604 sagemaker-training-toolkit INFO     Failed to parse hyperparameter rot90 value True to Json.\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m Returning the value itself\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m 2021-03-17 22:57:08,625 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m 2021-03-17 22:57:08,625 sagemaker-training-toolkit INFO     Failed to parse hyperparameter wandb_key value ded96d05c0cfafc1f209276af6c21cb7ac61e5de to Json.\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m Returning the value itself\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m 2021-03-17 22:57:08,626 sagemaker-training-toolkit INFO     Failed to parse hyperparameter s3_chkpt_dir value ckpt to Json.\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m Returning the value itself\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m 2021-03-17 22:57:08,626 sagemaker-training-toolkit INFO     Failed to parse hyperparameter augment value False to Json.\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m Returning the value itself\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m 2021-03-17 22:57:08,626 sagemaker-training-toolkit INFO     Failed to parse hyperparameter flip_left_right value False to Json.\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m Returning the value itself\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m 2021-03-17 22:57:08,626 sagemaker-training-toolkit INFO     Failed to parse hyperparameter flip_up_down value False to Json.\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m Returning the value itself\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m 2021-03-17 22:57:08,626 sagemaker-training-toolkit INFO     Failed to parse hyperparameter rot90 value True to Json.\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m Returning the value itself\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m 2021-03-17 22:57:08,639 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m \n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m Training Env:\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m \n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m {\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m     \"additional_framework_parameters\": {},\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m     \"channel_input_dirs\": {\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m         \"data\": \"/opt/ml/input/data/data\"\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m     },\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m     \"current_host\": \"algo-1-39mtm\",\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m     \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m     \"hosts\": [\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m         \"algo-1-39mtm\"\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m     ],\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m     \"hyperparameters\": {\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m         \"wandb_key\": \"ded96d05c0cfafc1f209276af6c21cb7ac61e5de\",\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m         \"epochs\": 10,\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m         \"s3_chkpt_dir\": \"ckpt\",\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m         \"augment\": \"False\",\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m         \"flip_left_right\": \"False\",\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m         \"flip_up_down\": \"False\",\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m         \"rot90\": \"True\"\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m     },\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m     \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m     \"input_data_config\": {\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m         \"data\": {\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m         }\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m     },\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m     \"input_dir\": \"/opt/ml/input\",\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m     \"is_master\": true,\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m     \"job_name\": \"tf-custom-container-test-2021-03-17-22-57-03-811\",\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m     \"log_level\": 20,\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m     \"master_hostname\": \"algo-1-39mtm\",\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m     \"model_dir\": \"/opt/ml/model\",\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m     \"module_dir\": \"/opt/ml/code\",\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m     \"module_name\": \"train\",\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m     \"network_interface_name\": \"eth0\",\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m     \"num_cpus\": 2,\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m     \"num_gpus\": 0,\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m     \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m     \"output_dir\": \"/opt/ml/output\",\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m     \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m     \"resource_config\": {\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m         \"current_host\": \"algo-1-39mtm\",\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m         \"hosts\": [\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m             \"algo-1-39mtm\"\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m         ]\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m     },\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m     \"user_entry_point\": \"train.py\"\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m }\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m \n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m Environment variables:\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m \n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m SM_HOSTS=[\"algo-1-39mtm\"]\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m SM_NETWORK_INTERFACE_NAME=eth0\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m SM_HPS={\"augment\":\"False\",\"epochs\":10,\"flip_left_right\":\"False\",\"flip_up_down\":\"False\",\"rot90\":\"True\",\"s3_chkpt_dir\":\"ckpt\",\"wandb_key\":\"ded96d05c0cfafc1f209276af6c21cb7ac61e5de\"}\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m SM_USER_ENTRY_POINT=train.py\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m SM_FRAMEWORK_PARAMS={}\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m SM_RESOURCE_CONFIG={\"current_host\":\"algo-1-39mtm\",\"hosts\":[\"algo-1-39mtm\"]}\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m SM_INPUT_DATA_CONFIG={\"data\":{\"TrainingInputMode\":\"File\"}}\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m SM_CHANNELS=[\"data\"]\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m SM_CURRENT_HOST=algo-1-39mtm\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m SM_MODULE_NAME=train\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m SM_LOG_LEVEL=20\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m SM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m SM_INPUT_DIR=/opt/ml/input\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m SM_OUTPUT_DIR=/opt/ml/output\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m SM_NUM_CPUS=2\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m SM_NUM_GPUS=0\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m SM_MODEL_DIR=/opt/ml/model\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m SM_MODULE_DIR=/opt/ml/code\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"data\":\"/opt/ml/input/data/data\"},\"current_host\":\"algo-1-39mtm\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1-39mtm\"],\"hyperparameters\":{\"augment\":\"False\",\"epochs\":10,\"flip_left_right\":\"False\",\"flip_up_down\":\"False\",\"rot90\":\"True\",\"s3_chkpt_dir\":\"ckpt\",\"wandb_key\":\"ded96d05c0cfafc1f209276af6c21cb7ac61e5de\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"data\":{\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"tf-custom-container-test-2021-03-17-22-57-03-811\",\"log_level\":20,\"master_hostname\":\"algo-1-39mtm\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"/opt/ml/code\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":2,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-39mtm\",\"hosts\":[\"algo-1-39mtm\"]},\"user_entry_point\":\"train.py\"}\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m SM_USER_ARGS=[\"--augment\",\"False\",\"--epochs\",\"10\",\"--flip_left_right\",\"False\",\"--flip_up_down\",\"False\",\"--rot90\",\"True\",\"--s3_chkpt_dir\",\"ckpt\",\"--wandb_key\",\"ded96d05c0cfafc1f209276af6c21cb7ac61e5de\"]\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m SM_CHANNEL_DATA=/opt/ml/input/data/data\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m SM_HP_WANDB_KEY=ded96d05c0cfafc1f209276af6c21cb7ac61e5de\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m SM_HP_EPOCHS=10\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m SM_HP_S3_CHKPT_DIR=ckpt\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m SM_HP_AUGMENT=False\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m SM_HP_FLIP_LEFT_RIGHT=False\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m SM_HP_FLIP_UP_DOWN=False\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m SM_HP_ROT90=True\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m PYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python37.zip:/usr/local/lib/python3.7:/usr/local/lib/python3.7/lib-dynload:/usr/local/lib/python3.7/site-packages\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m \n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m Invoking script with the following command:\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m \n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m /usr/local/bin/python3.7 train.py --augment False --epochs 10 --flip_left_right False --flip_up_down False --rot90 True --s3_chkpt_dir ckpt --wandb_key ded96d05c0cfafc1f209276af6c21cb7ac61e5de\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m \n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m \n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m Please install GPU version of TF\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m \n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m False\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m False\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m False\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m True\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m Starting checkpoint: None\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m <class 'bool'> False\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m <class 'bool'> False\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m <class 'bool'> False\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m <class 'bool'> True\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m batch size 64, learning_rate 0.001, augment False\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m The file /opt/ml/input/data/data/labels_train.csv is missing positive labels for classes ['0', '2', '3', '7', '8', '9']\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m The file /opt/ml/input/data/data/labels_val.csv is missing positive labels for classes ['5', '9']\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m No data augmentation. Please set augment to True if you want to augment training dataset\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m Training on 100 images\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m Validation on 366 images \n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m [2021-03-17 22:57:15.427 b4f03978bad5:18 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m [2021-03-17 22:57:15.529 b4f03978bad5:18 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94773248/94765736 [==============================] - 2s 0us/step\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m No previous checkpoint found in opt/ml/checkpoints directory; start training from scratch\n",
      "\u001b[36mikx8w1hx57-algo-1-39mtm |\u001b[0m Epoch 1/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-c9d5f1afef21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    659\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_for_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_TrainingJob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mstart_new\u001b[0;34m(cls, estimator, inputs, experiment_config)\u001b[0m\n\u001b[1;32m   1422\u001b[0m         \"\"\"\n\u001b[1;32m   1423\u001b[0m         \u001b[0mtrain_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_train_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1424\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_job_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_mode, input_config, role, job_name, output_config, resource_config, vpc_config, hyperparameters, stop_condition, tags, metric_definitions, enable_network_isolation, image_uri, algorithm_arn, encrypt_inter_container_traffic, use_spot_instances, checkpoint_s3_uri, checkpoint_local_path, experiment_config, debugger_rule_configs, debugger_hook_config, tensorboard_output_config, enable_sagemaker_metrics, profiler_rule_configs, profiler_config)\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating training-job with name: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train request: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_training_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m     def _get_train_request(  # noqa: C901\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/sagemaker/local/local_session.py\u001b[0m in \u001b[0;36mcreate_training_job\u001b[0;34m(self, TrainingJobName, AlgorithmSpecification, OutputDataConfig, ResourceConfig, InputDataConfig, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mhyperparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"HyperParameters\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"HyperParameters\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting training job\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0mtraining_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInputDataConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOutputDataConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingJobName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mLocalSagemakerClient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_training_jobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTrainingJobName\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_job\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/sagemaker/local/entities.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, input_data_config, output_data_config, hyperparameters, job_name)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         self.model_artifacts = self.container.train(\n\u001b[0;32m--> 221\u001b[0;31m             \u001b[0minput_data_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_data_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m         )\n\u001b[1;32m    223\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/sagemaker/local/image.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_data_config, output_data_config, hyperparameters, job_name)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m             \u001b[0m_stream_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m             \u001b[0;31m# _stream_output() doesn't have the command line. We will handle the exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/sagemaker/local/image.py\u001b[0m in \u001b[0;36m_stream_output\u001b[0;34m(process)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mexit_code\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mstdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mexit_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "estimator = Estimator(image_uri='tf-custom-container-test',\n",
    "                      role='arn:aws:iam::963659202518:role/service-role/AmazonSageMaker-ExecutionRole-20210306T191865',\n",
    "                      instance_count=1,\n",
    "                      instance_type='local',\n",
    "                     hyperparameters=hyperparameters)\n",
    "\n",
    "\n",
    "estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Publish Container to ECR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "Sending build context to Docker daemon    151kB\n",
      "Step 1/6 : FROM 763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-training:2.4.1-gpu-py37-cu110-ubuntu18.04\n",
      " ---> 993791d9475c\n",
      "Step 2/6 : ENV PATH=\"/opt/ml/code:${PATH}\"\n",
      " ---> Using cache\n",
      " ---> 24f6ef66670c\n",
      "Step 3/6 : RUN pip3 install rasterio wandb tensorflow-addons\n",
      " ---> Using cache\n",
      " ---> 2e0e92ca4149\n",
      "Step 4/6 : COPY cb_feature_train3_aws.py /opt/ml/code/train.py\n",
      " ---> Using cache\n",
      " ---> cf475618a1f9\n",
      "Step 5/6 : COPY data_loader.py /opt/ml/code/data_loader.py\n",
      " ---> Using cache\n",
      " ---> e3d24cf93a48\n",
      "Step 6/6 : ENV SAGEMAKER_PROGRAM train.py\n",
      " ---> Using cache\n",
      " ---> 73562464146b\n",
      "Successfully built 73562464146b\n",
      "Successfully tagged pc-tf-custom-container-test:latest\n",
      "The push refers to repository [963659202518.dkr.ecr.us-east-1.amazonaws.com/pc-tf-custom-container-test]\n",
      "22e88d6da3e3: Preparing\n",
      "cf11aff593d0: Preparing\n",
      "800c99fed61a: Preparing\n",
      "0c867ea799c5: Preparing\n",
      "3132871073ea: Preparing\n",
      "f1b83ac14212: Preparing\n",
      "6466a3f31741: Preparing\n",
      "9e3e77dcebd5: Preparing\n",
      "696dcfd17105: Preparing\n",
      "3af524677961: Preparing\n",
      "1984b588e81e: Preparing\n",
      "79afeb064ec4: Preparing\n",
      "07f0cc670425: Preparing\n",
      "d10be926da6c: Preparing\n",
      "5307cb34159c: Preparing\n",
      "e71b841fa46d: Preparing\n",
      "ec188198b092: Preparing\n",
      "63b55db384b2: Preparing\n",
      "45a33cd0da2c: Preparing\n",
      "36b80b2c0a82: Preparing\n",
      "7568fbcf322a: Preparing\n",
      "1dad3611ca5c: Preparing\n",
      "61b14b75ab48: Preparing\n",
      "a4f997b6ac61: Preparing\n",
      "c9e9bb4bc893: Preparing\n",
      "9b89fb942b77: Preparing\n",
      "53194dce1444: Preparing\n",
      "ef8330bcc944: Preparing\n",
      "964ee116c0c0: Preparing\n",
      "7a694df0ad6c: Preparing\n",
      "3fd9df553184: Preparing\n",
      "805802706667: Preparing\n",
      "63b55db384b2: Waiting\n",
      "45a33cd0da2c: Waiting\n",
      "36b80b2c0a82: Waiting\n",
      "7568fbcf322a: Waiting\n",
      "1dad3611ca5c: Waiting\n",
      "61b14b75ab48: Waiting\n",
      "a4f997b6ac61: Waiting\n",
      "c9e9bb4bc893: Waiting\n",
      "9b89fb942b77: Waiting\n",
      "53194dce1444: Waiting\n",
      "ef8330bcc944: Waiting\n",
      "964ee116c0c0: Waiting\n",
      "7a694df0ad6c: Waiting\n",
      "805802706667: Waiting\n",
      "3fd9df553184: Waiting\n",
      "1984b588e81e: Waiting\n",
      "79afeb064ec4: Waiting\n",
      "07f0cc670425: Waiting\n",
      "f1b83ac14212: Waiting\n",
      "d10be926da6c: Waiting\n",
      "6466a3f31741: Waiting\n",
      "5307cb34159c: Waiting\n",
      "e71b841fa46d: Waiting\n",
      "ec188198b092: Waiting\n",
      "9e3e77dcebd5: Waiting\n",
      "696dcfd17105: Waiting\n",
      "3af524677961: Waiting\n",
      "3132871073ea: Layer already exists\n",
      "0c867ea799c5: Layer already exists\n",
      "f1b83ac14212: Layer already exists\n",
      "6466a3f31741: Layer already exists\n",
      "9e3e77dcebd5: Layer already exists\n",
      "696dcfd17105: Layer already exists\n",
      "3af524677961: Layer already exists\n",
      "1984b588e81e: Layer already exists\n",
      "07f0cc670425: Layer already exists\n",
      "d10be926da6c: Layer already exists\n",
      "5307cb34159c: Layer already exists\n",
      "cf11aff593d0: Pushed\n",
      "e71b841fa46d: Layer already exists\n",
      "22e88d6da3e3: Pushed\n",
      "ec188198b092: Layer already exists\n",
      "63b55db384b2: Layer already exists\n",
      "45a33cd0da2c: Layer already exists\n",
      "36b80b2c0a82: Layer already exists\n",
      "7568fbcf322a: Layer already exists\n",
      "1dad3611ca5c: Layer already exists\n",
      "61b14b75ab48: Layer already exists\n",
      "c9e9bb4bc893: Layer already exists\n",
      "a4f997b6ac61: Layer already exists\n",
      "9b89fb942b77: Layer already exists\n",
      "53194dce1444: Layer already exists\n",
      "ef8330bcc944: Layer already exists\n",
      "964ee116c0c0: Layer already exists\n",
      "7a694df0ad6c: Layer already exists\n",
      "3fd9df553184: Layer already exists\n",
      "805802706667: Layer already exists\n",
      "79afeb064ec4: Layer already exists\n",
      "800c99fed61a: Pushed\n",
      "latest: digest: sha256:b825a033b797e5d32b768b0b393059ce4db0183ea6ce782d249c86d46b240b38 size: 7030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "# Specify an algorithm name\n",
    "algorithm_name=pc-tf-custom-container-test\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-east-1}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "\n",
    "$(aws ecr get-login --region ${region} --no-include-email)\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "docker build -t ${algorithm_name} .\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'963659202518.dkr.ecr.us-east-1.amazonaws.com/pc-tf-custom-container-test:latest'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "ecr_repository = 'pc-tf-custom-container-test'\n",
    "tag = ':latest'\n",
    "\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "uri_suffix = 'amazonaws.com'\n",
    "if region in ['cn-north-1', 'cn-northwest-1']:\n",
    "    uri_suffix = 'amazonaws.com.cn'\n",
    "\n",
    "image_uri = '{}.dkr.ecr.{}.{}/{}'.format(account_id, region, uri_suffix, ecr_repository + tag)\n",
    "\n",
    "image_uri\n",
    "# This should return something like\n",
    "# 111122223333.dkr.ecr.us-east-2.amazonaws.com/sagemaker-byoc-test:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For ECR Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Data Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': 's3://canopy-production-ml/training_inputs/train_val_full/'}\n",
      "{'wandb_key': 'ded96d05c0cfafc1f209276af6c21cb7ac61e5de', 'epochs': '2', 's3_chkpt_dir': 'ckpt', 'batch_size': '100'}\n"
     ]
    }
   ],
   "source": [
    "# S3 directories\n",
    "\n",
    "training_files = \"s3://canopy-production-ml/training_inputs/train_val_full/\"\n",
    "# val_file = \"s3://canopy-production-ml/training_inputs/val_labels.csv\"\n",
    "# labels_file = \"s3://canopy-production-ml/training_inputs/labels.json\"\n",
    "\n",
    "inputs = {\"data\":training_files}\n",
    "# hyperparameters = {\"wandb_key\":\"abfa0dec9fc06fbfa6392496f40a22a8d47e58cf\",\n",
    "#                    \"epochs\":\"20\",\n",
    "#                    \"s3_chkpt_dir\":\"ckpt\",\n",
    "#                    \"batch_size\":\"20\",}\n",
    "\n",
    "hyperparameters = {\n",
    "    \"wandb_key\": \"ded96d05c0cfafc1f209276af6c21cb7ac61e5de\",\n",
    "    \"epochs\": \"2\",\n",
    "    \"s3_chkpt_dir\": \"ckpt\",\n",
    "    \"batch_size\": \"100\",\n",
    "    #\"last_checkpoint\": \"ckpt/pc-tf-custom-container-test-job-2021-03-12-23-55-45-390/model_resnet_epoch_3.h5\"\n",
    "}\n",
    "\n",
    "print(inputs)\n",
    "print(hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiler_config=ProfilerConfig(\n",
    "    framework_profile_params=FrameworkProfile(start_unix_time=int(time.time()), duration=600)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = 'pc-tf-custom-container-test-job-no-augment'\n",
    "\n",
    "# create estimator\n",
    "estimator = Estimator(image_uri=image_uri,\n",
    "                       instance_type='ml.p3.16xlarge',\n",
    "                       output_path='s3://canopy-production-ml-output',\n",
    "                       base_job_name=job_name,\n",
    "                       instance_count=1,\n",
    "                       role=get_execution_role(), # Passes to the container the AWS role that you are using on this notebook\n",
    "                       py_version='py37',\n",
    "                     profiler_config=profiler_config,\n",
    "                     checkpoint_s3_uri=\n",
    "                      f's3://canopy-production-ml-output/ckpt/{job_name}-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())}',\n",
    "                     hyperparameters=hyperparameters,\n",
    "                      max_wait=60*60*24*3,\n",
    "                      max_run=60*60*24*3,\n",
    "                      use_spot_instances=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-17 20:41:04 Starting - Starting the training job...\n",
      "2021-03-17 20:41:11 Starting - Launching requested ML instancesProfilerReport-1616013664: InProgress\n",
      "............\n",
      "2021-03-17 20:43:36 Starting - Preparing the instances for training......\n",
      "2021-03-17 20:44:36 Downloading - Downloading input data...\n",
      "2021-03-17 20:44:56 Training - Downloading the training image.......................\u001b[34m2021-03-17 20:48:41.311304: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m2021-03-17 20:48:41.317805: I tensorflow/core/profiler/internal/smprofiler_config_reader.cc:123] PID of the process that is writing to the timeline : 1\u001b[0m\n",
      "\u001b[34m2021-03-17 20:48:41.318571: I tensorflow/core/profiler/internal/smprofiler_timeline.cc:121] SageMaker Profiler Timeline Writer read the following config parameters :\u001b[0m\n",
      "\u001b[34m2021-03-17 20:48:41.318589: I tensorflow/core/profiler/internal/smprofiler_timeline.cc:122] Base Folder : /opt/ml/output/profiler\u001b[0m\n",
      "\u001b[34m2021-03-17 20:48:41.318595: I tensorflow/core/profiler/internal/smprofiler_timeline.cc:123] Node Id : 1-algo-1\u001b[0m\n",
      "\u001b[34m2021-03-17 20:48:41.318602: I tensorflow/core/profiler/internal/smprofiler_timeline.cc:124] Maximum file size : 10485760\u001b[0m\n",
      "\u001b[34m2021-03-17 20:48:41.318608: I tensorflow/core/profiler/internal/smprofiler_timeline.cc:125] Close file interval : 60\u001b[0m\n",
      "\u001b[34m2021-03-17 20:48:41.318613: I tensorflow/core/profiler/internal/smprofiler_timeline.cc:126] Continuous fail count threshold : 50\u001b[0m\n",
      "\u001b[34m2021-03-17 20:48:41.318622: I tensorflow/core/profiler/internal/smprofiler_timeline.cc:144] PID of the current Timeline Process : 1\u001b[0m\n",
      "\u001b[34m2021-03-17 20:48:41.413900: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\u001b[0m\n",
      "\u001b[34m2021-03-17 20:48:41.522979: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m2021-03-17 20:48:45,950 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[34m2021-03-17 20:48:45,950 sagemaker-training-toolkit INFO     Failed to parse hyperparameter wandb_key value ded96d05c0cfafc1f209276af6c21cb7ac61e5de to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2021-03-17 20:48:45,950 sagemaker-training-toolkit INFO     Failed to parse hyperparameter s3_chkpt_dir value ckpt to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2021-03-17 20:48:46,110 sagemaker-training-toolkit INFO     Failed to parse hyperparameter wandb_key value ded96d05c0cfafc1f209276af6c21cb7ac61e5de to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2021-03-17 20:48:46,110 sagemaker-training-toolkit INFO     Failed to parse hyperparameter s3_chkpt_dir value ckpt to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2021-03-17 20:48:46,199 sagemaker-training-toolkit INFO     Failed to parse hyperparameter wandb_key value ded96d05c0cfafc1f209276af6c21cb7ac61e5de to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2021-03-17 20:48:46,199 sagemaker-training-toolkit INFO     Failed to parse hyperparameter s3_chkpt_dir value ckpt to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2021-03-17 20:48:46,282 sagemaker-training-toolkit INFO     Failed to parse hyperparameter wandb_key value ded96d05c0cfafc1f209276af6c21cb7ac61e5de to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2021-03-17 20:48:46,282 sagemaker-training-toolkit INFO     Failed to parse hyperparameter s3_chkpt_dir value ckpt to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2021-03-17 20:48:46,299 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"data\": \"/opt/ml/input/data/data\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch_size\": 100,\n",
      "        \"wandb_key\": \"ded96d05c0cfafc1f209276af6c21cb7ac61e5de\",\n",
      "        \"epochs\": 2,\n",
      "        \"s3_chkpt_dir\": \"ckpt\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"data\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pc-tf-custom-container-test-job-no-augm-2021-03-17-20-41-04-183\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"/opt/ml/code\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 64,\n",
      "    \"num_gpus\": 8,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch_size\":100,\"epochs\":2,\"s3_chkpt_dir\":\"ckpt\",\"wandb_key\":\"ded96d05c0cfafc1f209276af6c21cb7ac61e5de\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"data\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"data\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=64\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=/opt/ml/code\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"data\":\"/opt/ml/input/data/data\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch_size\":100,\"epochs\":2,\"s3_chkpt_dir\":\"ckpt\",\"wandb_key\":\"ded96d05c0cfafc1f209276af6c21cb7ac61e5de\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"data\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pc-tf-custom-container-test-job-no-augm-2021-03-17-20-41-04-183\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"/opt/ml/code\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch_size\",\"100\",\"--epochs\",\"2\",\"--s3_chkpt_dir\",\"ckpt\",\"--wandb_key\",\"ded96d05c0cfafc1f209276af6c21cb7ac61e5de\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_DATA=/opt/ml/input/data/data\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH_SIZE=100\u001b[0m\n",
      "\u001b[34mSM_HP_WANDB_KEY=ded96d05c0cfafc1f209276af6c21cb7ac61e5de\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=2\u001b[0m\n",
      "\u001b[34mSM_HP_S3_CHKPT_DIR=ckpt\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python37.zip:/usr/local/lib/python3.7:/usr/local/lib/python3.7/lib-dynload:/usr/local/lib/python3.7/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/local/bin/python3.7 train.py --batch_size 100 --epochs 2 --s3_chkpt_dir ckpt --wandb_key ded96d05c0cfafc1f209276af6c21cb7ac61e5de\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "2021-03-17 20:48:57 Training - Training image download completed. Training in progress.\u001b[34mDefault GPU Device: /device:GPU:0\u001b[0m\n",
      "\u001b[34mStarting checkpoint: None\u001b[0m\n",
      "\u001b[34mbatch size 100, learning_rate 0.001, augment False\u001b[0m\n",
      "\u001b[34mThe file /opt/ml/input/data/data/labels_train.csv is missing positive labels for classes []\u001b[0m\n",
      "\u001b[34mThe file /opt/ml/input/data/data/labels_val.csv is missing positive labels for classes []\u001b[0m\n",
      "\u001b[34mNo data augmentation. Please set augment to True if you want to augment training dataset\u001b[0m\n",
      "\u001b[34mTraining on 27270 images\u001b[0m\n",
      "\u001b[34mValidation on 6795 images \u001b[0m\n",
      "\u001b[34m[2021-03-17 20:49:01.324 ip-10-2-196-78.ec2.internal:143 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2021-03-17 20:49:01.413 ip-10-2-196-78.ec2.internal:143 INFO profiler_config_parser.py:102] Using config at /opt/ml/input/config/profilerconfig.json.\u001b[0m\n",
      "\u001b[34mDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\u001b[0m\n",
      "\u001b[34m#015    8192/94765736 [..............................] - ETA: 10s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015  417792/94765736 [..............................] - ETA: 11s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015  827392/94765736 [..............................] - ETA: 11s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015 1548288/94765736 [..............................] - ETA: 9s #010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015 2768896/94765736 [..............................] - ETA: 6s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015 4005888/94765736 [>.............................] - ETA: 5s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015 5242880/94765736 [>.............................] - ETA: 5s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015 6479872/94765736 [=>............................] - ETA: 4s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015 7716864/94765736 [=>............................] - ETA: 4s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#015 8962048/94765736 [=>............................] - ETA: 4s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01510199040/94765736 [==>...........................] - ETA: 4s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01511436032/94765736 [==>...........................] - ETA: 4s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01512681216/94765736 [===>..........................] - ETA: 3s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01513918208/94765736 [===>..........................] - ETA: 3s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01515155200/94765736 [===>..........................] - ETA: 3s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01516400384/94765736 [====>.........................] - ETA: 3s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01517637376/94765736 [====>.........................] - ETA: 3s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01518874368/94765736 [====>.........................] - ETA: 3s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01520119552/94765736 [=====>........................] - ETA: 3s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01521356544/94765736 [=====>........................] - ETA: 3s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01522593536/94765736 [======>.......................] - ETA: 3s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01523838720/94765736 [======>.......................] - ETA: 3s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01525075712/94765736 [======>.......................] - ETA: 3s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01526312704/94765736 [=======>......................] - ETA: 3s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01527549696/94765736 [=======>......................] - ETA: 2s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01528778496/94765736 [========>.....................] - ETA: 2s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01530015488/94765736 [========>.....................] - ETA: 2s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01531260672/94765736 [========>.....................] - ETA: 2s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01532497664/94765736 [=========>....................] - ETA: 2s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01533734656/94765736 [=========>....................] - ETA: 2s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01534979840/94765736 [==========>...................] - ETA: 2s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01536216832/94765736 [==========>...................] - ETA: 2s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01537453824/94765736 [==========>...................] - ETA: 2s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01538690816/94765736 [===========>..................] - ETA: 2s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01539927808/94765736 [===========>..................] - ETA: 2s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01541164800/94765736 [============>.................] - ETA: 2s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01542393600/94765736 [============>.................] - ETA: 2s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01543630592/94765736 [============>.................] - ETA: 2s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01544867584/94765736 [=============>................] - ETA: 2s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01546104576/94765736 [=============>................] - ETA: 2s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01547341568/94765736 [=============>................] - ETA: 2s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01548578560/94765736 [==============>...............] - ETA: 1s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01549815552/94765736 [==============>...............] - ETA: 1s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01551044352/94765736 [===============>..............] - ETA: 1s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01552281344/94765736 [===============>..............] - ETA: 1s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01553518336/94765736 [===============>..............] - ETA: 1s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01554755328/94765736 [================>.............] - ETA: 1s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01556000512/94765736 [================>.............] - ETA: 1s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01557237504/94765736 [=================>............] - ETA: 1s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01558474496/94765736 [=================>............] - ETA: 1s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01559719680/94765736 [=================>............] - ETA: 1s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01560956672/94765736 [==================>...........] - ETA: 1s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01562193664/94765736 [==================>...........] - ETA: 1s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01563430656/94765736 [===================>..........] - ETA: 1s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01564667648/94765736 [===================>..........] - ETA: 1s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01565904640/94765736 [===================>..........] - ETA: 1s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01567141632/94765736 [====================>.........] - ETA: 1s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01568386816/94765736 [====================>.........] - ETA: 1s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01569623808/94765736 [=====================>........] - ETA: 1s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01570860800/94765736 [=====================>........] - ETA: 0s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01572097792/94765736 [=====================>........] - ETA: 0s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01573334784/94765736 [======================>.......] - ETA: 0s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01574571776/94765736 [======================>.......] - ETA: 0s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01575800576/94765736 [======================>.......] - ETA: 0s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01577037568/94765736 [=======================>......] - ETA: 0s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01578274560/94765736 [=======================>......] - ETA: 0s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01579511552/94765736 [========================>.....] - ETA: 0s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01580748544/94765736 [========================>.....] - ETA: 0s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01581985536/94765736 [========================>.....] - ETA: 0s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01583214336/94765736 [=========================>....] - ETA: 0s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01584451328/94765736 [=========================>....] - ETA: 0s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01585688320/94765736 [==========================>...] - ETA: 0s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01586925312/94765736 [==========================>...] - ETA: 0s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01588154112/94765736 [==========================>...] - ETA: 0s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01589391104/94765736 [===========================>..] - ETA: 0s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01590628096/94765736 [===========================>..] - ETA: 0s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01591865088/94765736 [============================>.] - ETA: 0s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01593110272/94765736 [============================>.] - ETA: 0s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01594347264/94765736 [============================>.] - ETA: 0s#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#01594773248/94765736 [==============================] - 4s 0us/step\u001b[0m\n",
      "\u001b[34mNo previous checkpoint found in opt/ml/checkpoints directory; start training from scratch\u001b[0m\n",
      "\u001b[34mEpoch 1/2\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-0731e612c7b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compilation_job_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   1589\u001b[0m         \u001b[0;31m# If logs are requested, call logs_for_jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"None\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1591\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1592\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1593\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type)\u001b[0m\n\u001b[1;32m   3593\u001b[0m                 \u001b[0mpositions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3594\u001b[0m                 \u001b[0mdot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3595\u001b[0;31m                 \u001b[0mcolor_wrap\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3596\u001b[0m             )\n\u001b[1;32m   3597\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mLogState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOMPLETE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_flush_log_streams\u001b[0;34m(stream_names, instance_count, client, log_group, job_name, positions, dot, color_wrap)\u001b[0m\n\u001b[1;32m   4480\u001b[0m                 \u001b[0mlogStreamNamePrefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4481\u001b[0m                 \u001b[0morderBy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"LogStreamName\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4482\u001b[0;31m                 \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4483\u001b[0m             )\n\u001b[1;32m   4484\u001b[0m             \u001b[0mstream_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"logStreamName\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstreams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"logStreams\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    356\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m             http, parsed_response = self._make_request(\n\u001b[0;32m--> 663\u001b[0;31m                 operation_model, request_dict, request_context)\n\u001b[0m\u001b[1;32m    664\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m         self.meta.events.emit(\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, operation_model, request_dict, request_context)\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_endpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m             self.meta.events.emit(\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/botocore/endpoint.py\u001b[0m in \u001b[0;36mmake_request\u001b[0;34m(self, operation_model, request_dict)\u001b[0m\n\u001b[1;32m    100\u001b[0m         logger.debug(\"Making request for %s with params: %s\",\n\u001b[1;32m    101\u001b[0m                      operation_model, request_dict)\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/botocore/endpoint.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, request_dict, operation_model)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'context'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         success_response, exception = self._get_response(\n\u001b[0;32m--> 135\u001b[0;31m             request, operation_model, context)\n\u001b[0m\u001b[1;32m    136\u001b[0m         while self._needs_retry(attempts, operation_model, request_dict,\n\u001b[1;32m    137\u001b[0m                                 success_response, exception):\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/botocore/endpoint.py\u001b[0m in \u001b[0;36m_get_response\u001b[0;34m(self, request, operation_model, context)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;31m# If no exception occurs then exception is None.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         success_response, exception = self._do_get_response(\n\u001b[0;32m--> 167\u001b[0;31m             request, operation_model)\n\u001b[0m\u001b[1;32m    168\u001b[0m         kwargs_to_emit = {\n\u001b[1;32m    169\u001b[0m             \u001b[0;34m'response_dict'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/botocore/endpoint.py\u001b[0m in \u001b[0;36m_do_get_response\u001b[0;34m(self, request, operation_model)\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0mhttp_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_non_none_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhttp_response\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m                 \u001b[0mhttp_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPClientError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/botocore/endpoint.py\u001b[0m in \u001b[0;36m_send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_send\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/botocore/httpsession.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    318\u001b[0m                 \u001b[0mpreload_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m                 \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                 \u001b[0mchunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_chunked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m             )\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 706\u001b[0;31m                 \u001b[0mchunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunked\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    707\u001b[0m             )\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    443\u001b[0m                     \u001b[0;31m# Python 3 (including for exceptions like SystemExit).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m                     \u001b[0;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m                     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    438\u001b[0m                 \u001b[0;31m# Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m                     \u001b[0;31m# Remove the TypeError from the exception chain in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1377\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1379\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1380\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1010\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1012\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Read on closed or unwrapped SSL socket.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    629\u001b[0m         \"\"\"\n\u001b[1;32m    630\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profiler Viewing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://canopy-production-ml-output'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pc-tf-custom-container-test-job-2021-03-12-21-36-45-193'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.latest_training_job.job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"job_name\":\"pc-tf-custom-container-test-job-2021-03-12-21-36-45-193\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_output_path = estimator.output_path + \"/\"+ estimator.latest_training_job.job_name + \"/rule-output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://canopy-production-ml-output/pc-tf-custom-container-test-job-2021-03-09-06-30-02-520/rule-output'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rule_output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-09 06:45:11     350803 pc-tf-custom-container-test-job-2021-03-09-06-30-02-520/rule-output/ProfilerReport-1615271402/profiler-output/profiler-report.html\n",
      "2021-03-09 06:45:11     202858 pc-tf-custom-container-test-job-2021-03-09-06-30-02-520/rule-output/ProfilerReport-1615271402/profiler-output/profiler-report.ipynb\n",
      "2021-03-09 06:45:07        192 pc-tf-custom-container-test-job-2021-03-09-06-30-02-520/rule-output/ProfilerReport-1615271402/profiler-output/profiler-reports/BatchSize.json\n",
      "2021-03-09 06:45:07      53300 pc-tf-custom-container-test-job-2021-03-09-06-30-02-520/rule-output/ProfilerReport-1615271402/profiler-output/profiler-reports/CPUBottleneck.json\n",
      "2021-03-09 06:45:07        126 pc-tf-custom-container-test-job-2021-03-09-06-30-02-520/rule-output/ProfilerReport-1615271402/profiler-output/profiler-reports/Dataloader.json\n",
      "2021-03-09 06:45:07        130 pc-tf-custom-container-test-job-2021-03-09-06-30-02-520/rule-output/ProfilerReport-1615271402/profiler-output/profiler-reports/GPUMemoryIncrease.json\n",
      "2021-03-09 06:45:07       2581 pc-tf-custom-container-test-job-2021-03-09-06-30-02-520/rule-output/ProfilerReport-1615271402/profiler-output/profiler-reports/IOBottleneck.json\n",
      "2021-03-09 06:45:07        307 pc-tf-custom-container-test-job-2021-03-09-06-30-02-520/rule-output/ProfilerReport-1615271402/profiler-output/profiler-reports/LoadBalancing.json\n",
      "2021-03-09 06:45:07        331 pc-tf-custom-container-test-job-2021-03-09-06-30-02-520/rule-output/ProfilerReport-1615271402/profiler-output/profiler-reports/LowGPUUtilization.json\n",
      "2021-03-09 06:45:07        179 pc-tf-custom-container-test-job-2021-03-09-06-30-02-520/rule-output/ProfilerReport-1615271402/profiler-output/profiler-reports/MaxInitializationTime.json\n",
      "2021-03-09 06:45:07        133 pc-tf-custom-container-test-job-2021-03-09-06-30-02-520/rule-output/ProfilerReport-1615271402/profiler-output/profiler-reports/OverallFrameworkMetrics.json\n",
      "2021-03-09 06:45:07        612 pc-tf-custom-container-test-job-2021-03-09-06-30-02-520/rule-output/ProfilerReport-1615271402/profiler-output/profiler-reports/OverallSystemUsage.json\n",
      "2021-03-09 06:45:07        156 pc-tf-custom-container-test-job-2021-03-09-06-30-02-520/rule-output/ProfilerReport-1615271402/profiler-output/profiler-reports/StepOutlier.json\n"
     ]
    }
   ],
   "source": [
    "! aws s3 ls {rule_output_path} --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_s3_obj(s3_key):\n",
    "    s3 = boto3.resource('s3')\n",
    "    obj = s3.Object('canopy-production-ml-output', s3_key)\n",
    "    obj_bytes = io.BytesIO(obj.get()['Body'].read())\n",
    "    return obj_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'pc-tf-custom-container-test-job-2021-03-07-19-37-23-579/rule-output/ProfilerReport-1615145843/profiler-output/profiler-reports/Dataloader.json'\n",
    "data = json.load(read_s3_obj(key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'RuleTriggered': 0,\n",
       " 'Violations': 0,\n",
       " 'Details': {},\n",
       " 'Datapoints': 0,\n",
       " 'RuleParameters': 'min_threshold:70\\nmax_threshold:200'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search for H5 Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3 = boto3.resource('s3')\n",
    "my_bucket = s3.Bucket('canopy-production-ml-output')\n",
    "files = my_bucket.objects.all()\n",
    "file_list = []\n",
    "for file in files:\n",
    "    if file.key.endswith('.h5'):\n",
    "         file_list.append(file.key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/cb_feature_detection/sagemaker_staging/docker_test_folder\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cb_feature_Launch_Training_Job2.ipynb  docker_test_folder   labels_test_v1.csv\n",
      "cb_feature_Launch_Training_Job3.ipynb  entry_point_test.py  test_script.py\n",
      "cb_feature_train1_aws.py\t       labels.json\t    val_labels.csv\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/ec2-user/SageMaker/cb_feature_detection/sagemaker_staging/val_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'getvalue'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-cc9ef4caf6b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcsv_buffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0ms3_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboto3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m's3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0ms3_resource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ckpt/test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcsv_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'getvalue'"
     ]
    }
   ],
   "source": [
    "from io import StringIO # python3; python2: BytesIO \n",
    "import boto3\n",
    "\n",
    "bucket = 'canopy-production-ml-output' # already created on S3\n",
    "csv_buffer = StringIO()\n",
    "csv_buffer = df.to_csv(csv_buffer)\n",
    "s3_resource = boto3.resource('s3')\n",
    "s3_resource.Object(bucket, 'ckpt/test.csv').put(Body=csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module sagemaker.session in sagemaker:\n",
      "\n",
      "NAME\n",
      "    sagemaker.session - Placeholder docstring\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        LogState\n",
      "        Session\n",
      "    \n",
      "    class LogState(builtins.object)\n",
      "     |  Placeholder docstring\n",
      "     |  \n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  COMPLETE = 5\n",
      "     |  \n",
      "     |  JOB_COMPLETE = 4\n",
      "     |  \n",
      "     |  STARTING = 1\n",
      "     |  \n",
      "     |  TAILING = 3\n",
      "     |  \n",
      "     |  WAIT_IN_PROGRESS = 2\n",
      "    \n",
      "    class Session(builtins.object)\n",
      "     |  Manage interactions with the Amazon SageMaker APIs and any other AWS services needed.\n",
      "     |  \n",
      "     |  This class provides convenient methods for manipulating entities and resources that Amazon\n",
      "     |  SageMaker uses, such as training jobs, endpoints, and input datasets in S3.\n",
      "     |  AWS service calls are delegated to an underlying Boto3 session, which by default\n",
      "     |  is initialized using the AWS configuration chain. When you make an Amazon SageMaker API call\n",
      "     |  that accesses an S3 bucket location and one is not specified, the ``Session`` creates a default\n",
      "     |  bucket based on a naming convention which includes the current AWS account ID.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, boto_session=None, sagemaker_client=None, sagemaker_runtime_client=None, sagemaker_featurestore_runtime_client=None, default_bucket=None)\n",
      "     |      Initialize a SageMaker ``Session``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          boto_session (boto3.session.Session): The underlying Boto3 session which AWS service\n",
      "     |              calls are delegated to (default: None). If not provided, one is created with\n",
      "     |              default AWS configuration chain.\n",
      "     |          sagemaker_client (boto3.SageMaker.Client): Client which makes Amazon SageMaker service\n",
      "     |              calls other than ``InvokeEndpoint`` (default: None). Estimators created using this\n",
      "     |              ``Session`` use this client. If not provided, one will be created using this\n",
      "     |              instance's ``boto_session``.\n",
      "     |          sagemaker_runtime_client (boto3.SageMakerRuntime.Client): Client which makes\n",
      "     |              ``InvokeEndpoint`` calls to Amazon SageMaker (default: None). Predictors created\n",
      "     |              using this ``Session`` use this client. If not provided, one will be created using\n",
      "     |              this instance's ``boto_session``.\n",
      "     |          sagemaker_featurestore_runtime_client (boto3.SageMakerFeatureStoreRuntime.Client):\n",
      "     |              Client which makes SageMaker FeatureStore record related calls to Amazon SageMaker\n",
      "     |              (default: None). If not provided, one will be created using\n",
      "     |              this instance's ``boto_session``.\n",
      "     |          default_bucket (str): The default Amazon S3 bucket to be used by this session.\n",
      "     |              This will be created the next time an Amazon S3 bucket is needed (by calling\n",
      "     |              :func:`default_bucket`).\n",
      "     |              If not provided, a default bucket will be created based on the following format:\n",
      "     |              \"sagemaker-{region}-{aws-account-id}\".\n",
      "     |              Example: \"sagemaker-my-custom-bucket\".\n",
      "     |  \n",
      "     |  account_id(self) -> str\n",
      "     |      Get the AWS account id of the caller.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          AWS account ID.\n",
      "     |  \n",
      "     |  auto_ml(self, input_config, output_config, auto_ml_job_config, role, job_name, problem_type=None, job_objective=None, generate_candidate_definitions_only=False, tags=None)\n",
      "     |      Create an Amazon SageMaker AutoML job.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          input_config (list[dict]): A list of Channel objects. Each channel contains \"DataSource\"\n",
      "     |              and \"TargetAttributeName\", \"CompressionType\" is an optional field.\n",
      "     |          output_config (dict): The S3 URI where you want to store the training results and\n",
      "     |              optional KMS key ID.\n",
      "     |          auto_ml_job_config (dict): A dict of AutoMLJob config, containing \"StoppingCondition\",\n",
      "     |              \"SecurityConfig\", optionally contains \"VolumeKmsKeyId\".\n",
      "     |          role (str): The Amazon Resource Name (ARN) of an IAM role that\n",
      "     |              Amazon SageMaker can assume to perform tasks on your behalf.\n",
      "     |          job_name (str): A string that can be used to identify an AutoMLJob. Each AutoMLJob\n",
      "     |              should have a unique job name.\n",
      "     |          problem_type (str): The type of problem of this AutoMLJob. Valid values are\n",
      "     |              \"Regression\", \"BinaryClassification\", \"MultiClassClassification\". If None,\n",
      "     |              SageMaker AutoMLJob will infer the problem type automatically.\n",
      "     |          job_objective (dict): AutoMLJob objective, contains \"AutoMLJobObjectiveType\" (optional),\n",
      "     |              \"MetricName\" and \"Value\".\n",
      "     |          generate_candidate_definitions_only (bool): Indicates whether to only generate candidate\n",
      "     |              definitions. If True, AutoML.list_candidates() cannot be called. Default: False.\n",
      "     |          tags ([dict[str,str]]): A list of dictionaries containing key-value\n",
      "     |              pairs.\n",
      "     |  \n",
      "     |  compile_model(self, input_model_config, output_model_config, role, job_name, stop_condition, tags)\n",
      "     |      Create an Amazon SageMaker Neo compilation job.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          input_model_config (dict): the trained model and the Amazon S3 location where it is\n",
      "     |              stored.\n",
      "     |          output_model_config (dict): Identifies the Amazon S3 location where you want Amazon\n",
      "     |              SageMaker Neo to save the results of compilation job\n",
      "     |          role (str): An AWS IAM role (either name or full ARN). The Amazon SageMaker Neo\n",
      "     |              compilation jobs use this role to access model artifacts. You must grant\n",
      "     |              sufficient permissions to this role.\n",
      "     |          job_name (str): Name of the compilation job being created.\n",
      "     |          stop_condition (dict): Defines when compilation job shall finish. Contains entries\n",
      "     |              that can be understood by the service like ``MaxRuntimeInSeconds``.\n",
      "     |          tags (list[dict]): List of tags for labeling a compile model job. For more, see\n",
      "     |              https://docs.aws.amazon.com/sagemaker/latest/dg/API_Tag.html.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          str: ARN of the compile model job, if it is created.\n",
      "     |  \n",
      "     |  create_endpoint(self, endpoint_name, config_name, tags=None, wait=True)\n",
      "     |      Create an Amazon SageMaker ``Endpoint`` according to the configuration in the request.\n",
      "     |      \n",
      "     |      Once the ``Endpoint`` is created, client applications can send requests to obtain\n",
      "     |      inferences. The endpoint configuration is created using the ``CreateEndpointConfig`` API.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          endpoint_name (str): Name of the Amazon SageMaker ``Endpoint`` being created.\n",
      "     |          config_name (str): Name of the Amazon SageMaker endpoint configuration to deploy.\n",
      "     |          wait (bool): Whether to wait for the endpoint deployment to complete before returning\n",
      "     |              (default: True).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          str: Name of the Amazon SageMaker ``Endpoint`` created.\n",
      "     |  \n",
      "     |  create_endpoint_config(self, name, model_name, initial_instance_count, instance_type, accelerator_type=None, tags=None, kms_key=None, data_capture_config_dict=None)\n",
      "     |      Create an Amazon SageMaker endpoint configuration.\n",
      "     |      \n",
      "     |      The endpoint configuration identifies the Amazon SageMaker model (created using the\n",
      "     |      ``CreateModel`` API) and the hardware configuration on which to deploy the model. Provide\n",
      "     |      this endpoint configuration to the ``CreateEndpoint`` API, which then launches the\n",
      "     |      hardware and deploys the model.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (str): Name of the Amazon SageMaker endpoint configuration to create.\n",
      "     |          model_name (str): Name of the Amazon SageMaker ``Model``.\n",
      "     |          initial_instance_count (int): Minimum number of EC2 instances to launch. The actual\n",
      "     |              number of active instances for an endpoint at any given time varies due to\n",
      "     |              autoscaling.\n",
      "     |          instance_type (str): Type of EC2 instance to launch, for example, 'ml.c4.xlarge'.\n",
      "     |          accelerator_type (str): Type of Elastic Inference accelerator to attach to the\n",
      "     |              instance. For example, 'ml.eia1.medium'.\n",
      "     |              For more information: https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html\n",
      "     |          tags(List[dict[str, str]]): Optional. The list of tags to add to the endpoint config.\n",
      "     |          kms_key (str): The KMS key that is used to encrypt the data on the storage volume\n",
      "     |              attached to the instance hosting the endpoint.\n",
      "     |          data_capture_config_dict (dict): Specifies configuration related to Endpoint data\n",
      "     |              capture for use with Amazon SageMaker Model Monitoring. Default: None.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |          >>> tags = [{'Key': 'tagname', 'Value': 'tagvalue'}]\n",
      "     |          For more information about tags, see\n",
      "     |          https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.add_tags\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          str: Name of the endpoint point configuration created.\n",
      "     |  \n",
      "     |  create_endpoint_config_from_existing(self, existing_config_name, new_config_name, new_tags=None, new_kms_key=None, new_data_capture_config_dict=None, new_production_variants=None)\n",
      "     |      Create an Amazon SageMaker endpoint configuration from an existing one.\n",
      "     |      \n",
      "     |      It also updates any values that were passed in.\n",
      "     |      The endpoint configuration identifies the Amazon SageMaker model (created using the\n",
      "     |      ``CreateModel`` API) and the hardware configuration on which to deploy the model. Provide\n",
      "     |      this endpoint configuration to the ``CreateEndpoint`` API, which then launches the\n",
      "     |      hardware and deploys the model.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          new_config_name (str): Name of the Amazon SageMaker endpoint configuration to create.\n",
      "     |          existing_config_name (str): Name of the existing Amazon SageMaker endpoint\n",
      "     |              configuration.\n",
      "     |          new_tags (list[dict[str, str]]): Optional. The list of tags to add to the endpoint\n",
      "     |              config. If not specified, the tags of the existing endpoint configuration are used.\n",
      "     |              If any of the existing tags are reserved AWS ones (i.e. begin with \"aws\"),\n",
      "     |              they are not carried over to the new endpoint configuration.\n",
      "     |          new_kms_key (str): The KMS key that is used to encrypt the data on the storage volume\n",
      "     |              attached to the instance hosting the endpoint (default: None). If not specified,\n",
      "     |              the KMS key of the existing endpoint configuration is used.\n",
      "     |          new_data_capture_config_dict (dict): Specifies configuration related to Endpoint data\n",
      "     |              capture for use with Amazon SageMaker Model Monitoring (default: None).\n",
      "     |              If not specified, the data capture configuration of the existing\n",
      "     |              endpoint configuration is used.\n",
      "     |          new_production_variants (list[dict]): The configuration for which model(s) to host and\n",
      "     |              the resources to deploy for hosting the model(s). If not specified,\n",
      "     |              the ``ProductionVariants`` of the existing endpoint configuration is used.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          str: Name of the endpoint point configuration created.\n",
      "     |  \n",
      "     |  create_feature_group(self, feature_group_name:str, record_identifier_name:str, event_time_feature_name:str, feature_definitions:Sequence[Dict[str, str]], role_arn:str, online_store_config:Dict[str, str]=None, offline_store_config:Dict[str, str]=None, description:str=None, tags:List[Dict[str, str]]=None) -> Dict[str, Any]\n",
      "     |      Creates a FeatureGroup in the FeatureStore service.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          feature_group_name (str): name of the FeatureGroup.\n",
      "     |          record_identifier_name (str): name of the record identifier feature.\n",
      "     |          event_time_feature_name (str): name of the event time feature.\n",
      "     |          feature_definitions (Sequence[Dict[str, str]]): list of feature definitions.\n",
      "     |          role_arn (str): ARN of the role will be used to execute the api.\n",
      "     |          online_store_config (Dict[str, str]): dict contains configuration of the\n",
      "     |              feature online store.\n",
      "     |          offline_store_config (Dict[str, str]): dict contains configuration of the\n",
      "     |              feature offline store.\n",
      "     |          description (str): description of the FeatureGroup.\n",
      "     |          tags (List[Dict[str, str]]): list of tags for labeling a FeatureGroup.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Response dict from service.\n",
      "     |  \n",
      "     |  create_model(self, name, role, container_defs, vpc_config=None, enable_network_isolation=False, primary_container=None, tags=None)\n",
      "     |      Create an Amazon SageMaker ``Model``.\n",
      "     |      \n",
      "     |      Specify the S3 location of the model artifacts and Docker image containing\n",
      "     |      the inference code. Amazon SageMaker uses this information to deploy the\n",
      "     |      model in Amazon SageMaker. This method can also be used to create a Model for an Inference\n",
      "     |      Pipeline if you pass the list of container definitions through the containers parameter.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (str): Name of the Amazon SageMaker ``Model`` to create.\n",
      "     |          role (str): An AWS IAM role (either name or full ARN). The Amazon SageMaker training\n",
      "     |              jobs and APIs that create Amazon SageMaker endpoints use this role to access\n",
      "     |              training data and model artifacts. You must grant sufficient permissions to this\n",
      "     |              role.\n",
      "     |          container_defs (list[dict[str, str]] or [dict[str, str]]): A single container\n",
      "     |              definition or a list of container definitions which will be invoked sequentially\n",
      "     |              while performing the prediction. If the list contains only one container, then\n",
      "     |              it'll be passed to SageMaker Hosting as the ``PrimaryContainer`` and otherwise,\n",
      "     |              it'll be passed as ``Containers``.You can also specify the  return value of\n",
      "     |              ``sagemaker.get_container_def()`` or ``sagemaker.pipeline_container_def()``,\n",
      "     |              which will used to create more advanced container configurations, including model\n",
      "     |              containers which need artifacts from S3.\n",
      "     |          vpc_config (dict[str, list[str]]): The VpcConfig set on the model (default: None)\n",
      "     |              * 'Subnets' (list[str]): List of subnet ids.\n",
      "     |              * 'SecurityGroupIds' (list[str]): List of security group ids.\n",
      "     |          enable_network_isolation (bool): Whether the model requires network isolation or not.\n",
      "     |          primary_container (str or dict[str, str]): Docker image which defines the inference\n",
      "     |              code. You can also specify the return value of ``sagemaker.container_def()``,\n",
      "     |              which is used to create more advanced container configurations, including model\n",
      "     |              containers which need artifacts from S3. This field is deprecated, please use\n",
      "     |              container_defs instead.\n",
      "     |          tags(List[dict[str, str]]): Optional. The list of tags to add to the model.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |          >>> tags = [{'Key': 'tagname', 'Value': 'tagvalue'}]\n",
      "     |          For more information about tags, see https://boto3.amazonaws.com/v1/documentation            /api/latest/reference/services/sagemaker.html#SageMaker.Client.add_tags\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          str: Name of the Amazon SageMaker ``Model`` created.\n",
      "     |  \n",
      "     |  create_model_from_job(self, training_job_name, name=None, role=None, image_uri=None, model_data_url=None, env=None, vpc_config_override='VPC_CONFIG_DEFAULT', tags=None)\n",
      "     |      Create an Amazon SageMaker ``Model`` from a SageMaker Training Job.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          training_job_name (str): The Amazon SageMaker Training Job name.\n",
      "     |          name (str): The name of the SageMaker ``Model`` to create (default: None).\n",
      "     |              If not specified, the training job name is used.\n",
      "     |          role (str): The ``ExecutionRoleArn`` IAM Role ARN for the ``Model``, specified either\n",
      "     |              by an IAM role name or role ARN. If None, the ``RoleArn`` from the SageMaker\n",
      "     |              Training Job will be used.\n",
      "     |          image_uri (str): The Docker image URI (default: None). If None, it\n",
      "     |              defaults to the training image URI from ``training_job_name``.\n",
      "     |          model_data_url (str): S3 location of the model data (default: None). If None, defaults\n",
      "     |              to the ``ModelS3Artifacts`` of ``training_job_name``.\n",
      "     |          env (dict[string,string]): Model environment variables (default: {}).\n",
      "     |          vpc_config_override (dict[str, list[str]]): Optional override for VpcConfig set on the\n",
      "     |              model.\n",
      "     |              Default: use VpcConfig from training job.\n",
      "     |              * 'Subnets' (list[str]): List of subnet ids.\n",
      "     |              * 'SecurityGroupIds' (list[str]): List of security group ids.\n",
      "     |          tags(List[dict[str, str]]): Optional. The list of tags to add to the model.\n",
      "     |              For more, see https://docs.aws.amazon.com/sagemaker/latest/dg/API_Tag.html.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          str: The name of the created ``Model``.\n",
      "     |  \n",
      "     |  create_model_package_from_algorithm(self, name, description, algorithm_arn, model_data)\n",
      "     |      Create a SageMaker Model Package from the results of training with an Algorithm Package.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (str): ModelPackage name\n",
      "     |          description (str): Model Package description\n",
      "     |          algorithm_arn (str): arn or name of the algorithm used for training.\n",
      "     |          model_data (str): s3 URI to the model artifacts produced by training\n",
      "     |  \n",
      "     |  create_model_package_from_containers(self, containers=None, content_types=None, response_types=None, inference_instances=None, transform_instances=None, model_package_name=None, model_package_group_name=None, model_metrics=None, metadata_properties=None, marketplace_cert=False, approval_status='PendingManualApproval', description=None)\n",
      "     |      Get request dictionary for CreateModelPackage API.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          containers (list): A list of inference containers that can be used for inference\n",
      "     |              specifications of Model Package (default: None).\n",
      "     |          content_types (list): The supported MIME types for the input data (default: None).\n",
      "     |          response_types (list): The supported MIME types for the output data (default: None).\n",
      "     |          inference_instances (list): A list of the instance types that are used to\n",
      "     |              generate inferences in real-time (default: None).\n",
      "     |          transform_instances (list): A list of the instance types on which a transformation\n",
      "     |              job can be run or on which an endpoint can be deployed (default: None).\n",
      "     |          model_package_name (str): Model Package name, exclusive to `model_package_group_name`,\n",
      "     |              using `model_package_name` makes the Model Package un-versioned (default: None).\n",
      "     |          model_package_group_name (str): Model Package Group name, exclusive to\n",
      "     |              `model_package_name`, using `model_package_group_name` makes the Model Package\n",
      "     |              versioned (default: None).\n",
      "     |          model_metrics (ModelMetrics): ModelMetrics object (default: None).\n",
      "     |          metadata_properties (MetadataProperties): MetadataProperties object (default: None)\n",
      "     |          marketplace_cert (bool): A boolean value indicating if the Model Package is certified\n",
      "     |              for AWS Marketplace (default: False).\n",
      "     |          approval_status (str): Model Approval Status, values can be \"Approved\", \"Rejected\",\n",
      "     |              or \"PendingManualApproval\" (default: \"PendingManualApproval\").\n",
      "     |          description (str): Model Package description (default: None).\n",
      "     |  \n",
      "     |  create_monitoring_schedule(self, monitoring_schedule_name, schedule_expression, statistics_s3_uri, constraints_s3_uri, monitoring_inputs, monitoring_output_config, instance_count, instance_type, volume_size_in_gb, volume_kms_key, image_uri, entrypoint, arguments, record_preprocessor_source_uri, post_analytics_processor_source_uri, max_runtime_in_seconds, environment, network_config, role_arn, tags)\n",
      "     |      Create an Amazon SageMaker monitoring schedule.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          monitoring_schedule_name (str): The name of the monitoring schedule. The name must be\n",
      "     |              unique within an AWS Region in an AWS account. Names should have a minimum length\n",
      "     |              of 1 and a maximum length of 63 characters.\n",
      "     |          schedule_expression (str): The cron expression that dictates the monitoring execution\n",
      "     |              schedule.\n",
      "     |          statistics_s3_uri (str): The S3 uri of the statistics file to use.\n",
      "     |          constraints_s3_uri (str): The S3 uri of the constraints file to use.\n",
      "     |          monitoring_inputs ([dict]): List of MonitoringInput dictionaries.\n",
      "     |          monitoring_output_config (dict): A config dictionary, which contains a list of\n",
      "     |              MonitoringOutput dictionaries, as well as an optional KMS key ID.\n",
      "     |          instance_count (int): The number of instances to run.\n",
      "     |          instance_type (str): The type of instance to run.\n",
      "     |          volume_size_in_gb (int): Size of the volume in GB.\n",
      "     |          volume_kms_key (str): KMS key to use when encrypting the volume.\n",
      "     |          image_uri (str): The image uri to use for monitoring executions.\n",
      "     |          entrypoint (str): The entrypoint to the monitoring execution image.\n",
      "     |          arguments (str): The arguments to pass to the monitoring execution image.\n",
      "     |          record_preprocessor_source_uri (str or None): The S3 uri that points to the script that\n",
      "     |              pre-processes the dataset (only applicable to first-party images).\n",
      "     |          post_analytics_processor_source_uri (str or None): The S3 uri that points to the script\n",
      "     |              that post-processes the dataset (only applicable to first-party images).\n",
      "     |          max_runtime_in_seconds (int): Specifies a limit to how long\n",
      "     |              the processing job can run, in seconds.\n",
      "     |          environment (dict): Environment variables to start the monitoring execution\n",
      "     |              container with.\n",
      "     |          network_config (dict): Specifies networking options, such as network\n",
      "     |              traffic encryption between processing containers, whether to allow\n",
      "     |              inbound and outbound network calls to and from processing containers,\n",
      "     |              and VPC subnets and security groups to use for VPC-enabled processing\n",
      "     |              jobs.\n",
      "     |          role_arn (str): The Amazon Resource Name (ARN) of an IAM role that\n",
      "     |              Amazon SageMaker can assume to perform tasks on your behalf.\n",
      "     |          tags ([dict[str,str]]): A list of dictionaries containing key-value\n",
      "     |              pairs.\n",
      "     |  \n",
      "     |  create_tuning_job(self, job_name, tuning_config, training_config=None, training_config_list=None, warm_start_config=None, tags=None)\n",
      "     |      Create an Amazon SageMaker hyperparameter tuning job.\n",
      "     |      \n",
      "     |      This method supports creating tuning jobs with single or multiple training algorithms\n",
      "     |      (estimators), while the ``tune()`` method above only supports creating tuning jobs\n",
      "     |      with single training algorithm.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          job_name (str): Name of the tuning job being created.\n",
      "     |          tuning_config (dict): Configuration to launch the tuning job.\n",
      "     |          training_config (dict): Configuration to launch training jobs under the tuning job\n",
      "     |              using a single algorithm.\n",
      "     |          training_config_list (list[dict]): A list of configurations to launch training jobs\n",
      "     |              under the tuning job using one or multiple algorithms. Either training_config\n",
      "     |              or training_config_list should be provided, but not both.\n",
      "     |          warm_start_config (dict): Configuration defining the type of warm start and\n",
      "     |              other required configurations.\n",
      "     |          tags (list[dict]): List of tags for labeling the tuning job. For more, see\n",
      "     |              https://docs.aws.amazon.com/sagemaker/latest/dg/API_Tag.html.\n",
      "     |  \n",
      "     |  default_bucket(self)\n",
      "     |      Return the name of the default bucket to use in relevant Amazon SageMaker interactions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          str: The name of the default bucket, which is of the form:\n",
      "     |              ``sagemaker-{region}-{AWS account ID}``.\n",
      "     |  \n",
      "     |  delete_endpoint(self, endpoint_name)\n",
      "     |      Delete an Amazon SageMaker ``Endpoint``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          endpoint_name (str): Name of the Amazon SageMaker ``Endpoint`` to delete.\n",
      "     |  \n",
      "     |  delete_endpoint_config(self, endpoint_config_name)\n",
      "     |      Delete an Amazon SageMaker endpoint configuration.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          endpoint_config_name (str): Name of the Amazon SageMaker endpoint configuration to\n",
      "     |              delete.\n",
      "     |  \n",
      "     |  delete_feature_group(self, feature_group_name:str)\n",
      "     |      Deletes a FeatureGroup in the FeatureStore service.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          feature_group_name (str): name of the feature group to be deleted.\n",
      "     |  \n",
      "     |  delete_model(self, model_name)\n",
      "     |      Delete an Amazon SageMaker Model.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          model_name (str): Name of the Amazon SageMaker model to delete.\n",
      "     |  \n",
      "     |  delete_monitoring_schedule(self, monitoring_schedule_name)\n",
      "     |      Deletes a monitoring schedule.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          monitoring_schedule_name (str): The name of the Amazon SageMaker Monitoring\n",
      "     |              Schedule to delete.\n",
      "     |  \n",
      "     |  describe_auto_ml_job(self, job_name)\n",
      "     |      Calls the DescribeAutoMLJob API for the given job name and returns the response.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          job_name (str): The name of the AutoML job to describe.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict: A dictionary response with the AutoML Job description.\n",
      "     |  \n",
      "     |  describe_feature_group(self, feature_group_name:str, next_token:str=None) -> Dict[str, Any]\n",
      "     |      Describe a FeatureGroup by name in FeatureStore service.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          feature_group_name (str): name of the FeatureGroup to descibe.\n",
      "     |          next_token (str): next_token to get next page of features.\n",
      "     |      Returns:\n",
      "     |          Response dict from service.\n",
      "     |  \n",
      "     |  describe_model(self, name)\n",
      "     |      Calls the DescribeModel API for the given model name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (str): The name of the SageMaker model.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict: A dictionary response with the model description.\n",
      "     |  \n",
      "     |  describe_monitoring_schedule(self, monitoring_schedule_name)\n",
      "     |      Calls the DescribeMonitoringSchedule API for given name and returns the response.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          monitoring_schedule_name (str): The name of the processing job to describe.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict: A dictionary response with the processing job description.\n",
      "     |  \n",
      "     |  describe_processing_job(self, job_name)\n",
      "     |      Calls the DescribeProcessingJob API for the given job name and returns the response.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          job_name (str): The name of the processing job to describe.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict: A dictionary response with the processing job description.\n",
      "     |  \n",
      "     |  describe_training_job(self, job_name)\n",
      "     |      Calls the DescribeTrainingJob API for the given job name and returns the response.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          job_name (str): The name of the training job to describe.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict: A dictionary response with the training job description.\n",
      "     |  \n",
      "     |  describe_transform_job(self, job_name)\n",
      "     |      Calls the DescribeTransformJob API for the given job name and returns the response.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          job_name (str): The name of the transform job to describe.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict: A dictionary response with the transform job description.\n",
      "     |  \n",
      "     |  describe_tuning_job(self, job_name)\n",
      "     |      Calls DescribeHyperParameterTuningJob API for the given job name, returns the response.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          job_name (str): The name of the hyperparameter tuning job to describe.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict: A dictionary response with the hyperparameter tuning job description.\n",
      "     |  \n",
      "     |  download_athena_query_result(self, bucket:str, prefix:str, query_execution_id:str, filename:str)\n",
      "     |      Download query result file from S3.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          bucket (str): name of the S3 bucket where the result file is stored.\n",
      "     |          prefix (str): S3 prefix of the result file.\n",
      "     |          query_execution_id (str): execution ID of the Athena query.\n",
      "     |          filename (str): name of the downloaded file.\n",
      "     |  \n",
      "     |  download_data(self, path, bucket, key_prefix='', extra_args=None)\n",
      "     |      Download file or directory from S3.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          path (str): Local path where the file or directory should be downloaded to.\n",
      "     |          bucket (str): Name of the S3 Bucket to download from.\n",
      "     |          key_prefix (str): Optional S3 object key name prefix.\n",
      "     |          extra_args (dict): Optional extra arguments that may be passed to the\n",
      "     |              download operation. Please refer to the ExtraArgs parameter in the boto3\n",
      "     |              documentation here:\n",
      "     |              https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3-example-download-file.html\n",
      "     |  \n",
      "     |  endpoint_from_job(self, job_name, initial_instance_count, instance_type, image_uri=None, name=None, role=None, wait=True, model_environment_vars=None, vpc_config_override='VPC_CONFIG_DEFAULT', accelerator_type=None, data_capture_config=None)\n",
      "     |      Create an ``Endpoint`` using the results of a successful training job.\n",
      "     |      \n",
      "     |      Specify the job name, Docker image containing the inference code, and hardware\n",
      "     |      configuration to deploy the model. Internally the API, creates an Amazon SageMaker model\n",
      "     |      (that describes the model artifacts and the Docker image containing inference code),\n",
      "     |      endpoint configuration (describing the hardware to deploy for hosting the model), and\n",
      "     |      creates an ``Endpoint`` (launches the EC2 instances and deploys the model on them). In\n",
      "     |      response, the API returns the endpoint name to which you can send requests for inferences.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          job_name (str): Name of the training job to deploy the results of.\n",
      "     |          initial_instance_count (int): Minimum number of EC2 instances to launch. The actual\n",
      "     |              number of active instances for an endpoint at any given time varies due to\n",
      "     |              autoscaling.\n",
      "     |          instance_type (str): Type of EC2 instance to deploy to an endpoint for prediction,\n",
      "     |              for example, 'ml.c4.xlarge'.\n",
      "     |          image_uri (str): The Docker image which defines the inference code to be used\n",
      "     |              as the entry point for accepting prediction requests. If not specified, uses the\n",
      "     |              image used for the training job.\n",
      "     |          name (str): Name of the ``Endpoint`` to create. If not specified, uses the training job\n",
      "     |              name.\n",
      "     |          role (str): An AWS IAM role (either name or full ARN). The Amazon SageMaker training\n",
      "     |              jobs and APIs that create Amazon SageMaker endpoints use this role to access\n",
      "     |              training data and model artifacts. You must grant sufficient permissions to this\n",
      "     |              role.\n",
      "     |          wait (bool): Whether to wait for the endpoint deployment to complete before returning\n",
      "     |              (default: True).\n",
      "     |          model_environment_vars (dict[str, str]): Environment variables to set on the model\n",
      "     |              container (default: None).\n",
      "     |          vpc_config_override (dict[str, list[str]]): Overrides VpcConfig set on the model.\n",
      "     |              Default: use VpcConfig from training job.\n",
      "     |              * 'Subnets' (list[str]): List of subnet ids.\n",
      "     |              * 'SecurityGroupIds' (list[str]): List of security group ids.\n",
      "     |          accelerator_type (str): Type of Elastic Inference accelerator to attach to the\n",
      "     |              instance. For example, 'ml.eia1.medium'.\n",
      "     |              For more information: https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html\n",
      "     |          data_capture_config (sagemaker.model_monitor.DataCaptureConfig): Specifies\n",
      "     |              configuration related to Endpoint data capture for use with\n",
      "     |              Amazon SageMaker Model Monitoring. Default: None.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          str: Name of the ``Endpoint`` that is created.\n",
      "     |  \n",
      "     |  endpoint_from_model_data(self, model_s3_location, image_uri, initial_instance_count, instance_type, name=None, role=None, wait=True, model_environment_vars=None, model_vpc_config=None, accelerator_type=None, data_capture_config=None)\n",
      "     |      Create and deploy to an ``Endpoint`` using existing model data stored in S3.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          model_s3_location (str): S3 URI of the model artifacts to use for the endpoint.\n",
      "     |          image_uri (str): The Docker image URI which defines the runtime code to be\n",
      "     |              used as the entry point for accepting prediction requests.\n",
      "     |          initial_instance_count (int): Minimum number of EC2 instances to launch. The actual\n",
      "     |              number of active instances for an endpoint at any given time varies due to\n",
      "     |              autoscaling.\n",
      "     |          instance_type (str): Type of EC2 instance to deploy to an endpoint for prediction,\n",
      "     |              e.g. 'ml.c4.xlarge'.\n",
      "     |          name (str): Name of the ``Endpoint`` to create. If not specified, uses a name\n",
      "     |              generated by combining the image name with a timestamp.\n",
      "     |          role (str): An AWS IAM role (either name or full ARN). The Amazon SageMaker training\n",
      "     |              jobs and APIs that create Amazon SageMaker endpoints use this role to access\n",
      "     |              training data and model artifacts.\n",
      "     |              You must grant sufficient permissions to this role.\n",
      "     |          wait (bool): Whether to wait for the endpoint deployment to complete before returning\n",
      "     |              (default: True).\n",
      "     |          model_environment_vars (dict[str, str]): Environment variables to set on the model\n",
      "     |              container (default: None).\n",
      "     |          model_vpc_config (dict[str, list[str]]): The VpcConfig set on the model (default: None)\n",
      "     |              * 'Subnets' (list[str]): List of subnet ids.\n",
      "     |              * 'SecurityGroupIds' (list[str]): List of security group ids.\n",
      "     |          accelerator_type (str): Type of Elastic Inference accelerator to attach to the instance.\n",
      "     |              For example, 'ml.eia1.medium'.\n",
      "     |              For more information: https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html\n",
      "     |          data_capture_config (sagemaker.model_monitor.DataCaptureConfig): Specifies\n",
      "     |              configuration related to Endpoint data capture for use with\n",
      "     |              Amazon SageMaker Model Monitoring. Default: None.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          str: Name of the ``Endpoint`` that is created.\n",
      "     |  \n",
      "     |  endpoint_from_production_variants(self, name, production_variants, tags=None, kms_key=None, wait=True, data_capture_config_dict=None)\n",
      "     |      Create an SageMaker ``Endpoint`` from a list of production variants.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (str): The name of the ``Endpoint`` to create.\n",
      "     |          production_variants (list[dict[str, str]]): The list of production variants to deploy.\n",
      "     |          tags (list[dict[str, str]]): A list of key-value pairs for tagging the endpoint\n",
      "     |              (default: None).\n",
      "     |          kms_key (str): The KMS key that is used to encrypt the data on the storage volume\n",
      "     |              attached to the instance hosting the endpoint.\n",
      "     |          wait (bool): Whether to wait for the endpoint deployment to complete before returning\n",
      "     |              (default: True).\n",
      "     |          data_capture_config_dict (dict): Specifies configuration related to Endpoint data\n",
      "     |              capture for use with Amazon SageMaker Model Monitoring. Default: None.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          str: The name of the created ``Endpoint``.\n",
      "     |  \n",
      "     |  expand_role(self, role)\n",
      "     |      Expand an IAM role name into an ARN.\n",
      "     |      \n",
      "     |      If the role is already in the form of an ARN, then the role is simply returned. Otherwise\n",
      "     |      we retrieve the full ARN and return it.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          role (str): An AWS IAM role (either name or full ARN).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          str: The corresponding AWS IAM role ARN.\n",
      "     |  \n",
      "     |  get_caller_identity_arn(self)\n",
      "     |      Returns the ARN user or role whose credentials are used to call the API.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          str: The ARN user or role\n",
      "     |  \n",
      "     |  get_query_execution(self, query_execution_id:str) -> Dict[str, Any]\n",
      "     |      Get execution status of the Athena query.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          query_execution_id (str): execution ID of the Athena query.\n",
      "     |  \n",
      "     |  list_candidates(self, job_name, status_equals=None, candidate_name=None, candidate_arn=None, sort_order=None, sort_by=None, max_results=None)\n",
      "     |      Returns the list of candidates of an AutoML job for a given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          job_name (str): The name of the AutoML job. If None, will use object's\n",
      "     |              latest_auto_ml_job name.\n",
      "     |          status_equals (str): Filter the result with candidate status, values could be\n",
      "     |              \"Completed\", \"InProgress\", \"Failed\", \"Stopped\", \"Stopping\"\n",
      "     |          candidate_name (str): The name of a specified candidate to list.\n",
      "     |              Default to None.\n",
      "     |          candidate_arn (str): The Arn of a specified candidate to list.\n",
      "     |              Default to None.\n",
      "     |          sort_order (str): The order that the candidates will be listed in result.\n",
      "     |              Default to None.\n",
      "     |          sort_by (str): The value that the candidates will be sorted by.\n",
      "     |              Default to None.\n",
      "     |          max_results (int): The number of candidates will be listed in results,\n",
      "     |              between 1 to 100. Default to None. If None, will return all the candidates.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          list: A list of dictionaries with candidates information\n",
      "     |  \n",
      "     |  list_monitoring_executions(self, monitoring_schedule_name, sort_by='ScheduledTime', sort_order='Descending', max_results=100)\n",
      "     |      Lists the monitoring executions associated with the given monitoring_schedule_name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          monitoring_schedule_name (str): The monitoring_schedule_name for which to retrieve the\n",
      "     |              monitoring executions.\n",
      "     |          sort_by (str): The field to sort by. Can be one of: \"CreationTime\", \"ScheduledTime\",\n",
      "     |              \"Status\". Default: \"ScheduledTime\".\n",
      "     |          sort_order (str): The sort order. Can be one of: \"Ascending\", \"Descending\".\n",
      "     |              Default: \"Descending\".\n",
      "     |          max_results (int): The maximum number of results to return. Must be between 1 and 100.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict: Dictionary of monitoring schedule executions.\n",
      "     |  \n",
      "     |  list_monitoring_schedules(self, endpoint_name=None, sort_by='CreationTime', sort_order='Descending', max_results=100)\n",
      "     |      Lists the monitoring executions associated with the given monitoring_schedule_name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          endpoint_name (str): The name of the endpoint to filter on. If not provided, does not\n",
      "     |              filter on it. Default: None.\n",
      "     |          sort_by (str): The field to sort by. Can be one of: \"Name\", \"CreationTime\", \"Status\".\n",
      "     |              Default: \"CreationTime\".\n",
      "     |          sort_order (str): The sort order. Can be one of: \"Ascending\", \"Descending\".\n",
      "     |              Default: \"Descending\".\n",
      "     |          max_results (int): The maximum number of results to return. Must be between 1 and 100.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict: Dictionary of monitoring schedule executions.\n",
      "     |  \n",
      "     |  list_s3_files(self, bucket, key_prefix)\n",
      "     |      Lists the S3 files given an S3 bucket and key.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          bucket (str): Name of the S3 Bucket to download from.\n",
      "     |          key_prefix (str): S3 object key name prefix.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          [str]: The list of files at the S3 path.\n",
      "     |  \n",
      "     |  list_tags(self, resource_arn, max_results=50)\n",
      "     |      List the tags given an Amazon Resource Name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          resource_arn (str): The Amazon Resource Name (ARN) for which to get the tags list.\n",
      "     |          max_results (int): The maximum number of results to include in a single page.\n",
      "     |              This method takes care of that abstraction and returns a full list.\n",
      "     |  \n",
      "     |  logs_for_auto_ml_job(self, job_name, wait=False, poll=10)\n",
      "     |      Display logs for a given AutoML job, optionally tailing them until job is complete.\n",
      "     |      \n",
      "     |      If the output is a tty or a Jupyter cell, it will be color-coded\n",
      "     |      based on which instance the log entry is from.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          job_name (str): Name of the Auto ML job to display the logs for.\n",
      "     |          wait (bool): Whether to keep looking for new log entries until the job completes\n",
      "     |              (default: False).\n",
      "     |          poll (int): The interval in seconds between polling for new log entries and job\n",
      "     |              completion (default: 5).\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          exceptions.UnexpectedStatusException: If waiting and the training job fails.\n",
      "     |  \n",
      "     |  logs_for_job(self, job_name, wait=False, poll=10, log_type='All')\n",
      "     |      Display logs for a given training job, optionally tailing them until job is complete.\n",
      "     |      \n",
      "     |      If the output is a tty or a Jupyter cell, it will be color-coded\n",
      "     |      based on which instance the log entry is from.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          job_name (str): Name of the training job to display the logs for.\n",
      "     |          wait (bool): Whether to keep looking for new log entries until the job completes\n",
      "     |              (default: False).\n",
      "     |          poll (int): The interval in seconds between polling for new log entries and job\n",
      "     |              completion (default: 5).\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          exceptions.UnexpectedStatusException: If waiting and the training job fails.\n",
      "     |  \n",
      "     |  logs_for_processing_job(self, job_name, wait=False, poll=10)\n",
      "     |      Display logs for a given processing job, optionally tailing them until the is complete.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          job_name (str): Name of the processing job to display the logs for.\n",
      "     |          wait (bool): Whether to keep looking for new log entries until the job completes\n",
      "     |              (default: False).\n",
      "     |          poll (int): The interval in seconds between polling for new log entries and job\n",
      "     |              completion (default: 5).\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          ValueError: If the processing job fails.\n",
      "     |  \n",
      "     |  logs_for_transform_job(self, job_name, wait=False, poll=10)\n",
      "     |      Display logs for a given training job, optionally tailing them until job is complete.\n",
      "     |      \n",
      "     |      If the output is a tty or a Jupyter cell, it will be color-coded\n",
      "     |      based on which instance the log entry is from.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          job_name (str): Name of the transform job to display the logs for.\n",
      "     |          wait (bool): Whether to keep looking for new log entries until the job completes\n",
      "     |              (default: False).\n",
      "     |          poll (int): The interval in seconds between polling for new log entries and job\n",
      "     |              completion (default: 5).\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          ValueError: If the transform job fails.\n",
      "     |  \n",
      "     |  package_model_for_edge(self, output_model_config, role, job_name, compilation_job_name, model_name, model_version, resource_key, tags)\n",
      "     |      Create an Amazon SageMaker Edge packaging job.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          output_model_config (dict): Identifies the Amazon S3 location where you want Amazon\n",
      "     |              SageMaker Edge to save the results of edge packaging job\n",
      "     |          role (str): An AWS IAM role (either name or full ARN). The Amazon SageMaker Edge\n",
      "     |              edge packaging jobs use this role to access model artifacts. You must grant\n",
      "     |              sufficient permissions to this role.\n",
      "     |          job_name (str): Name of the edge packaging job being created.\n",
      "     |          compilation_job_name (str): Name of the compilation job being created.\n",
      "     |          resource_key (str): KMS key to encrypt the disk used to package the job\n",
      "     |          tags (list[dict]): List of tags for labeling a compile model job. For more, see\n",
      "     |              https://docs.aws.amazon.com/sagemaker/latest/dg/API_Tag.html.\n",
      "     |  \n",
      "     |  process(self, inputs, output_config, job_name, resources, stopping_condition, app_specification, environment, network_config, role_arn, tags, experiment_config=None)\n",
      "     |      Create an Amazon SageMaker processing job.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          inputs ([dict]): List of up to 10 ProcessingInput dictionaries.\n",
      "     |          output_config (dict): A config dictionary, which contains a list of up\n",
      "     |              to 10 ProcessingOutput dictionaries, as well as an optional KMS key ID.\n",
      "     |          job_name (str): The name of the processing job. The name must be unique\n",
      "     |              within an AWS Region in an AWS account. Names should have minimum\n",
      "     |              length of 1 and maximum length of 63 characters.\n",
      "     |          resources (dict): Encapsulates the resources, including ML instances\n",
      "     |              and storage, to use for the processing job.\n",
      "     |          stopping_condition (dict[str,int]): Specifies a limit to how long\n",
      "     |              the processing job can run, in seconds.\n",
      "     |          app_specification (dict[str,str]): Configures the processing job to\n",
      "     |              run the given image. Details are in the processing container\n",
      "     |              specification.\n",
      "     |          environment (dict): Environment variables to start the processing\n",
      "     |              container with.\n",
      "     |          network_config (dict): Specifies networking options, such as network\n",
      "     |              traffic encryption between processing containers, whether to allow\n",
      "     |              inbound and outbound network calls to and from processing containers,\n",
      "     |              and VPC subnets and security groups to use for VPC-enabled processing\n",
      "     |              jobs.\n",
      "     |          role_arn (str): The Amazon Resource Name (ARN) of an IAM role that\n",
      "     |              Amazon SageMaker can assume to perform tasks on your behalf.\n",
      "     |          tags ([dict[str,str]]): A list of dictionaries containing key-value\n",
      "     |              pairs.\n",
      "     |          experiment_config (dict): Experiment management configuration. Dictionary contains\n",
      "     |              three optional keys, 'ExperimentName', 'TrialName', and 'TrialComponentDisplayName'.\n",
      "     |              (default: ``None``)\n",
      "     |  \n",
      "     |  put_record(self, feature_group_name:str, record:Sequence[Dict[str, str]])\n",
      "     |      Puts a single record in the FeatureGroup.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          feature_group_name (str): name of the FeatureGroup.\n",
      "     |          record (Sequence[Dict[str, str]]): list of FeatureValue dicts to be ingested\n",
      "     |              into FeatureStore.\n",
      "     |  \n",
      "     |  read_s3_file(self, bucket, key_prefix)\n",
      "     |      Read a single file from S3.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          bucket (str): Name of the S3 Bucket to download from.\n",
      "     |          key_prefix (str): S3 object key name prefix.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          str: The body of the s3 file as a string.\n",
      "     |  \n",
      "     |  start_monitoring_schedule(self, monitoring_schedule_name)\n",
      "     |      Starts a monitoring schedule.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          monitoring_schedule_name (str): The name of the Amazon SageMaker Monitoring\n",
      "     |              Schedule to start.\n",
      "     |  \n",
      "     |  start_query_execution(self, catalog:str, database:str, query_string:str, output_location:str, kms_key:str=None) -> Dict[str, str]\n",
      "     |      Start Athena query execution.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          catalog (str): name of the data catalog.\n",
      "     |          database (str): name of the data catalog database.\n",
      "     |          query_string (str): SQL expression.\n",
      "     |          output_location (str): S3 location of the output file.\n",
      "     |          kms_key (str): KMS key id will be used to encrypt the result if given.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Response dict from the service.\n",
      "     |  \n",
      "     |  stop_monitoring_schedule(self, monitoring_schedule_name)\n",
      "     |      Stops a monitoring schedule.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          monitoring_schedule_name (str): The name of the Amazon SageMaker Monitoring\n",
      "     |              Schedule to stop.\n",
      "     |  \n",
      "     |  stop_processing_job(self, job_name)\n",
      "     |      Calls the StopProcessingJob API for the given job name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          job_name (str): The name of the processing job to stop.\n",
      "     |  \n",
      "     |  stop_training_job(self, job_name)\n",
      "     |      Calls the StopTrainingJob API for the given job name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          job_name (str): The name of the training job to stop.\n",
      "     |  \n",
      "     |  stop_transform_job(self, name)\n",
      "     |      Stop the Amazon SageMaker hyperparameter tuning job with the specified name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (str): Name of the Amazon SageMaker batch transform job.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          ClientError: If an error occurs while trying to stop the batch transform job.\n",
      "     |  \n",
      "     |  stop_tuning_job(self, name)\n",
      "     |      Stop the Amazon SageMaker hyperparameter tuning job with the specified name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (str): Name of the Amazon SageMaker hyperparameter tuning job.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          ClientError: If an error occurs while trying to stop the hyperparameter tuning job.\n",
      "     |  \n",
      "     |  train(self, input_mode, input_config, role, job_name, output_config, resource_config, vpc_config, hyperparameters, stop_condition, tags, metric_definitions, enable_network_isolation=False, image_uri=None, algorithm_arn=None, encrypt_inter_container_traffic=False, use_spot_instances=False, checkpoint_s3_uri=None, checkpoint_local_path=None, experiment_config=None, debugger_rule_configs=None, debugger_hook_config=None, tensorboard_output_config=None, enable_sagemaker_metrics=None, profiler_rule_configs=None, profiler_config=None)\n",
      "     |      Create an Amazon SageMaker training job.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          input_mode (str): The input mode that the algorithm supports. Valid modes:\n",
      "     |              * 'File' - Amazon SageMaker copies the training dataset from the S3 location to\n",
      "     |              a directory in the Docker container.\n",
      "     |              * 'Pipe' - Amazon SageMaker streams data directly from S3 to the container via a\n",
      "     |              Unix-named pipe.\n",
      "     |          input_config (list): A list of Channel objects. Each channel is a named input source.\n",
      "     |              Please refer to the format details described:\n",
      "     |              https://botocore.readthedocs.io/en/latest/reference/services/sagemaker.html#SageMaker.Client.create_training_job\n",
      "     |          role (str): An AWS IAM role (either name or full ARN). The Amazon SageMaker training\n",
      "     |              jobs and APIs that create Amazon SageMaker endpoints use this role to access\n",
      "     |              training data and model artifacts. You must grant sufficient permissions to this\n",
      "     |              role.\n",
      "     |          job_name (str): Name of the training job being created.\n",
      "     |          output_config (dict): The S3 URI where you want to store the training results and\n",
      "     |              optional KMS key ID.\n",
      "     |          resource_config (dict): Contains values for ResourceConfig:\n",
      "     |              * instance_count (int): Number of EC2 instances to use for training.\n",
      "     |              The key in resource_config is 'InstanceCount'.\n",
      "     |              * instance_type (str): Type of EC2 instance to use for training, for example,\n",
      "     |              'ml.c4.xlarge'. The key in resource_config is 'InstanceType'.\n",
      "     |          vpc_config (dict): Contains values for VpcConfig:\n",
      "     |              * subnets (list[str]): List of subnet ids.\n",
      "     |              The key in vpc_config is 'Subnets'.\n",
      "     |              * security_group_ids (list[str]): List of security group ids.\n",
      "     |              The key in vpc_config is 'SecurityGroupIds'.\n",
      "     |          hyperparameters (dict): Hyperparameters for model training. The hyperparameters are\n",
      "     |              made accessible as a dict[str, str] to the training code on SageMaker. For\n",
      "     |              convenience, this accepts other types for keys and values, but ``str()`` will be\n",
      "     |              called to convert them before training.\n",
      "     |          stop_condition (dict): Defines when training shall finish. Contains entries that can\n",
      "     |              be understood by the service like ``MaxRuntimeInSeconds``.\n",
      "     |          tags (list[dict]): List of tags for labeling a training job. For more, see\n",
      "     |              https://docs.aws.amazon.com/sagemaker/latest/dg/API_Tag.html.\n",
      "     |          metric_definitions (list[dict]): A list of dictionaries that defines the metric(s)\n",
      "     |              used to evaluate the training jobs. Each dictionary contains two keys: 'Name' for\n",
      "     |              the name of the metric, and 'Regex' for the regular expression used to extract the\n",
      "     |              metric from the logs.\n",
      "     |          enable_network_isolation (bool): Whether to request for the training job to run with\n",
      "     |              network isolation or not.\n",
      "     |          image_uri (str): Docker image containing training code.\n",
      "     |          algorithm_arn (str): Algorithm Arn from Marketplace.\n",
      "     |          encrypt_inter_container_traffic (bool): Specifies whether traffic between training\n",
      "     |              containers is encrypted for the training job (default: ``False``).\n",
      "     |          use_spot_instances (bool): whether to use spot instances for training.\n",
      "     |          checkpoint_s3_uri (str): The S3 URI in which to persist checkpoints\n",
      "     |              that the algorithm persists (if any) during training. (default:\n",
      "     |              ``None``).\n",
      "     |          checkpoint_local_path (str): The local path that the algorithm\n",
      "     |              writes its checkpoints to. SageMaker will persist all files\n",
      "     |              under this path to `checkpoint_s3_uri` continually during\n",
      "     |              training. On job startup the reverse happens - data from the\n",
      "     |              s3 location is downloaded to this path before the algorithm is\n",
      "     |              started. If the path is unset then SageMaker assumes the\n",
      "     |              checkpoints will be provided under `/opt/ml/checkpoints/`.\n",
      "     |              (default: ``None``).\n",
      "     |          experiment_config (dict): Experiment management configuration. Dictionary contains\n",
      "     |              three optional keys, 'ExperimentName', 'TrialName', and 'TrialComponentDisplayName'.\n",
      "     |              (default: ``None``)\n",
      "     |          enable_sagemaker_metrics (bool): enable SageMaker Metrics Time\n",
      "     |              Series. For more information see:\n",
      "     |              https://docs.aws.amazon.com/sagemaker/latest/dg/API_AlgorithmSpecification.html#SageMaker-Type-AlgorithmSpecification-EnableSageMakerMetricsTimeSeries\n",
      "     |              (default: ``None``).\n",
      "     |          profiler_rule_configs (list[dict]): A list of profiler rule configurations.\n",
      "     |          profiler_config (dict): Configuration for how profiling information is emitted\n",
      "     |              with SageMaker Profiler. (default: ``None``).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          str: ARN of the training job, if it is created.\n",
      "     |  \n",
      "     |  transform(self, job_name, model_name, strategy, max_concurrent_transforms, max_payload, env, input_config, output_config, resource_config, experiment_config, tags, data_processing, model_client_config=None)\n",
      "     |      Create an Amazon SageMaker transform job.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          job_name (str): Name of the transform job being created.\n",
      "     |          model_name (str): Name of the SageMaker model being used for the transform job.\n",
      "     |          strategy (str): The strategy used to decide how to batch records in a single request.\n",
      "     |              Possible values are 'MultiRecord' and 'SingleRecord'.\n",
      "     |          max_concurrent_transforms (int): The maximum number of HTTP requests to be made to\n",
      "     |              each individual transform container at one time.\n",
      "     |          max_payload (int): Maximum size of the payload in a single HTTP request to the\n",
      "     |              container in MB.\n",
      "     |          env (dict): Environment variables to be set for use during the transform job.\n",
      "     |          input_config (dict): A dictionary describing the input data (and its location) for the\n",
      "     |              job.\n",
      "     |          output_config (dict): A dictionary describing the output location for the job.\n",
      "     |          resource_config (dict): A dictionary describing the resources to complete the job.\n",
      "     |          experiment_config (dict): A dictionary describing the experiment configuration for the\n",
      "     |              job. Dictionary contains three optional keys,\n",
      "     |              'ExperimentName', 'TrialName', and 'TrialComponentDisplayName'.\n",
      "     |          tags (list[dict]): List of tags for labeling a transform job.\n",
      "     |          data_processing(dict): A dictionary describing config for combining the input data and\n",
      "     |              transformed data. For more, see\n",
      "     |              https://docs.aws.amazon.com/sagemaker/latest/dg/API_Tag.html.\n",
      "     |          model_client_config (dict): A dictionary describing the model configuration for the\n",
      "     |              job. Dictionary contains two optional keys,\n",
      "     |              'InvocationsTimeoutInSeconds', and 'InvocationsMaxRetries'.\n",
      "     |  \n",
      "     |  tune(self, job_name, strategy, objective_type, objective_metric_name, max_jobs, max_parallel_jobs, parameter_ranges, static_hyperparameters, input_mode, metric_definitions, role, input_config, output_config, resource_config, stop_condition, tags, warm_start_config, enable_network_isolation=False, image_uri=None, algorithm_arn=None, early_stopping_type='Off', encrypt_inter_container_traffic=False, vpc_config=None, use_spot_instances=False, checkpoint_s3_uri=None, checkpoint_local_path=None)\n",
      "     |      Create an Amazon SageMaker hyperparameter tuning job.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          job_name (str): Name of the tuning job being created.\n",
      "     |          strategy (str): Strategy to be used for hyperparameter estimations.\n",
      "     |          objective_type (str): The type of the objective metric for evaluating training jobs.\n",
      "     |              This value can be either 'Minimize' or 'Maximize'.\n",
      "     |          objective_metric_name (str): Name of the metric for evaluating training jobs.\n",
      "     |          max_jobs (int): Maximum total number of training jobs to start for the hyperparameter\n",
      "     |              tuning job.\n",
      "     |          max_parallel_jobs (int): Maximum number of parallel training jobs to start.\n",
      "     |          parameter_ranges (dict): Dictionary of parameter ranges. These parameter ranges can be\n",
      "     |              one of three types: Continuous, Integer, or Categorical.\n",
      "     |          static_hyperparameters (dict): Hyperparameters for model training. These\n",
      "     |              hyperparameters remain unchanged across all of the training jobs for the\n",
      "     |              hyperparameter tuning job. The hyperparameters are made accessible as a dictionary\n",
      "     |              for the training code on SageMaker.\n",
      "     |          image_uri (str): Docker image URI containing training code.\n",
      "     |          algorithm_arn (str): Resource ARN for training algorithm created on or subscribed from\n",
      "     |              AWS Marketplace (default: None).\n",
      "     |          input_mode (str): The input mode that the algorithm supports. Valid modes:\n",
      "     |              * 'File' - Amazon SageMaker copies the training dataset from the S3 location to\n",
      "     |              a directory in the Docker container.\n",
      "     |              * 'Pipe' - Amazon SageMaker streams data directly from S3 to the container via a\n",
      "     |              Unix-named pipe.\n",
      "     |          metric_definitions (list[dict]): A list of dictionaries that defines the metric(s)\n",
      "     |              used to evaluate the training jobs. Each dictionary contains two keys: 'Name' for\n",
      "     |              the name of the metric, and 'Regex' for the regular expression used to extract the\n",
      "     |              metric from the logs. This should be defined only for jobs that don't use an\n",
      "     |              Amazon algorithm.\n",
      "     |          role (str): An AWS IAM role (either name or full ARN). The Amazon SageMaker\n",
      "     |              training jobs and APIs that create Amazon SageMaker endpoints use this role to\n",
      "     |              access training data and model artifacts. You must grant sufficient permissions\n",
      "     |              to this role.\n",
      "     |          input_config (list): A list of Channel objects. Each channel is a named input source.\n",
      "     |              Please refer to the format details described:\n",
      "     |              https://botocore.readthedocs.io/en/latest/reference/services/sagemaker.html#SageMaker.Client.create_training_job\n",
      "     |          output_config (dict): The S3 URI where you want to store the training results and\n",
      "     |              optional KMS key ID.\n",
      "     |          resource_config (dict): Contains values for ResourceConfig:\n",
      "     |              * instance_count (int): Number of EC2 instances to use for training.\n",
      "     |              The key in resource_config is 'InstanceCount'.\n",
      "     |              * instance_type (str): Type of EC2 instance to use for training, for example,\n",
      "     |              'ml.c4.xlarge'. The key in resource_config is 'InstanceType'.\n",
      "     |          stop_condition (dict): When training should finish, e.g. ``MaxRuntimeInSeconds``.\n",
      "     |          tags (list[dict]): List of tags for labeling the tuning job. For more, see\n",
      "     |              https://docs.aws.amazon.com/sagemaker/latest/dg/API_Tag.html.\n",
      "     |          warm_start_config (dict): Configuration defining the type of warm start and\n",
      "     |              other required configurations.\n",
      "     |          early_stopping_type (str): Specifies whether early stopping is enabled for the job.\n",
      "     |              Can be either 'Auto' or 'Off'. If set to 'Off', early stopping will not be\n",
      "     |              attempted. If set to 'Auto', early stopping of some training jobs may happen, but\n",
      "     |              is not guaranteed to.\n",
      "     |          enable_network_isolation (bool): Specifies whether to isolate the training container\n",
      "     |              (default: ``False``).\n",
      "     |          encrypt_inter_container_traffic (bool): Specifies whether traffic between training\n",
      "     |              containers is encrypted for the training jobs started for this hyperparameter\n",
      "     |              tuning job (default: ``False``).\n",
      "     |          vpc_config (dict): Contains values for VpcConfig (default: None):\n",
      "     |              * subnets (list[str]): List of subnet ids.\n",
      "     |              The key in vpc_config is 'Subnets'.\n",
      "     |              * security_group_ids (list[str]): List of security group ids.\n",
      "     |              The key in vpc_config is 'SecurityGroupIds'.\n",
      "     |          use_spot_instances (bool): whether to use spot instances for training.\n",
      "     |          checkpoint_s3_uri (str): The S3 URI in which to persist checkpoints\n",
      "     |              that the algorithm persists (if any) during training. (default:\n",
      "     |              ``None``).\n",
      "     |          checkpoint_local_path (str): The local path that the algorithm\n",
      "     |              writes its checkpoints to. SageMaker will persist all files\n",
      "     |              under this path to `checkpoint_s3_uri` continually during\n",
      "     |              training. On job startup the reverse happens - data from the\n",
      "     |              s3 location is downloaded to this path before the algorithm is\n",
      "     |              started. If the path is unset then SageMaker assumes the\n",
      "     |              checkpoints will be provided under `/opt/ml/checkpoints/`.\n",
      "     |              (default: ``None``).\n",
      "     |  \n",
      "     |  update_endpoint(self, endpoint_name, endpoint_config_name, wait=True)\n",
      "     |      Update an Amazon SageMaker ``Endpoint`` , Raise an error endpoint_name does not exist.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          endpoint_name (str): Name of the Amazon SageMaker ``Endpoint`` to update.\n",
      "     |          endpoint_config_name (str): Name of the Amazon SageMaker endpoint configuration to\n",
      "     |              deploy.\n",
      "     |          wait (bool): Whether to wait for the endpoint deployment to complete before returning\n",
      "     |              (default: True).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          str: Name of the Amazon SageMaker ``Endpoint`` being updated.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          ValueError: if the endpoint does not already exist\n",
      "     |  \n",
      "     |  update_monitoring_schedule(self, monitoring_schedule_name, schedule_expression=None, statistics_s3_uri=None, constraints_s3_uri=None, monitoring_inputs=None, monitoring_output_config=None, instance_count=None, instance_type=None, volume_size_in_gb=None, volume_kms_key=None, image_uri=None, entrypoint=None, arguments=None, record_preprocessor_source_uri=None, post_analytics_processor_source_uri=None, max_runtime_in_seconds=None, environment=None, network_config=None, role_arn=None)\n",
      "     |      Update an Amazon SageMaker monitoring schedule.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          monitoring_schedule_name (str): The name of the monitoring schedule. The name must be\n",
      "     |              unique within an AWS Region in an AWS account. Names should have a minimum length\n",
      "     |              of 1 and a maximum length of 63 characters.\n",
      "     |          schedule_expression (str): The cron expression that dictates the monitoring execution\n",
      "     |              schedule.\n",
      "     |          statistics_s3_uri (str): The S3 uri of the statistics file to use.\n",
      "     |          constraints_s3_uri (str): The S3 uri of the constraints file to use.\n",
      "     |          monitoring_inputs ([dict]): List of MonitoringInput dictionaries.\n",
      "     |          monitoring_output_config (dict): A config dictionary, which contains a list of\n",
      "     |              MonitoringOutput dictionaries, as well as an optional KMS key ID.\n",
      "     |          instance_count (int): The number of instances to run.\n",
      "     |          instance_type (str): The type of instance to run.\n",
      "     |          volume_size_in_gb (int): Size of the volume in GB.\n",
      "     |          volume_kms_key (str): KMS key to use when encrypting the volume.\n",
      "     |          image_uri (str): The image uri to use for monitoring executions.\n",
      "     |          entrypoint (str): The entrypoint to the monitoring execution image.\n",
      "     |          arguments (str): The arguments to pass to the monitoring execution image.\n",
      "     |          record_preprocessor_source_uri (str or None): The S3 uri that points to the script that\n",
      "     |              pre-processes the dataset (only applicable to first-party images).\n",
      "     |          post_analytics_processor_source_uri (str or None): The S3 uri that points to the script\n",
      "     |              that post-processes the dataset (only applicable to first-party images).\n",
      "     |          max_runtime_in_seconds (int): Specifies a limit to how long\n",
      "     |              the processing job can run, in seconds.\n",
      "     |          environment (dict): Environment variables to start the monitoring execution\n",
      "     |              container with.\n",
      "     |          network_config (dict): Specifies networking options, such as network\n",
      "     |              traffic encryption between processing containers, whether to allow\n",
      "     |              inbound and outbound network calls to and from processing containers,\n",
      "     |              and VPC subnets and security groups to use for VPC-enabled processing\n",
      "     |              jobs.\n",
      "     |          role_arn (str): The Amazon Resource Name (ARN) of an IAM role that\n",
      "     |              Amazon SageMaker can assume to perform tasks on your behalf.\n",
      "     |          tags ([dict[str,str]]): A list of dictionaries containing key-value\n",
      "     |              pairs.\n",
      "     |  \n",
      "     |  update_training_job(self, job_name, profiler_rule_configs=None, profiler_config=None)\n",
      "     |      Calls the UpdateTrainingJob API for the given job name and returns the response.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          job_name (str): Name of the training job being updated.\n",
      "     |          profiler_rule_configs (list): List of profiler rule configurations. (default: ``None``).\n",
      "     |          profiler_config(dict): Configuration for how profiling information is emitted with\n",
      "     |              SageMaker Profiler. (default: ``None``).\n",
      "     |  \n",
      "     |  upload_data(self, path, bucket=None, key_prefix='data', extra_args=None)\n",
      "     |      Upload local file or directory to S3.\n",
      "     |      \n",
      "     |      If a single file is specified for upload, the resulting S3 object key is\n",
      "     |      ``{key_prefix}/{filename}`` (filename does not include the local path, if any specified).\n",
      "     |      If a directory is specified for upload, the API uploads all content, recursively,\n",
      "     |      preserving relative structure of subdirectories. The resulting object key names are:\n",
      "     |      ``{key_prefix}/{relative_subdirectory_path}/filename``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          path (str): Path (absolute or relative) of local file or directory to upload.\n",
      "     |          bucket (str): Name of the S3 Bucket to upload to (default: None). If not specified, the\n",
      "     |              default bucket of the ``Session`` is used (if default bucket does not exist, the\n",
      "     |              ``Session`` creates it).\n",
      "     |          key_prefix (str): Optional S3 object key name prefix (default: 'data'). S3 uses the\n",
      "     |              prefix to create a directory structure for the bucket content that it display in\n",
      "     |              the S3 console.\n",
      "     |          extra_args (dict): Optional extra arguments that may be passed to the upload operation.\n",
      "     |              Similar to ExtraArgs parameter in S3 upload_file function. Please refer to the\n",
      "     |              ExtraArgs parameter documentation here:\n",
      "     |              https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3-uploading-files.html#the-extraargs-parameter\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          str: The S3 URI of the uploaded file(s). If a file is specified in the path argument,\n",
      "     |              the URI format is: ``s3://{bucket name}/{key_prefix}/{original_file_name}``.\n",
      "     |              If a directory is specified in the path argument, the URI format is\n",
      "     |              ``s3://{bucket name}/{key_prefix}``.\n",
      "     |  \n",
      "     |  upload_string_as_file_body(self, body, bucket, key, kms_key=None)\n",
      "     |      Upload a string as a file body.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          body (str): String representing the body of the file.\n",
      "     |          bucket (str): Name of the S3 Bucket to upload to (default: None). If not specified, the\n",
      "     |              default bucket of the ``Session`` is used (if default bucket does not exist, the\n",
      "     |              ``Session`` creates it).\n",
      "     |          key (str): S3 object key. This is the s3 path to the file.\n",
      "     |          kms_key (str): The KMS key to use for encrypting the file.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          str: The S3 URI of the uploaded file.\n",
      "     |              The URI format is: ``s3://{bucket name}/{key}``.\n",
      "     |  \n",
      "     |  wait_for_athena_query(self, query_execution_id:str, poll:int=5)\n",
      "     |      Wait for Athena query to finish.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |           query_execution_id (str): execution ID of the Athena query.\n",
      "     |           poll (int): time interval to poll get_query_execution API.\n",
      "     |  \n",
      "     |  wait_for_auto_ml_job(self, job, poll=5)\n",
      "     |      Wait for an Amazon SageMaker AutoML job to complete.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          job (str): Name of the auto ml job to wait for.\n",
      "     |          poll (int): Polling interval in seconds (default: 5).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          (dict): Return value from the ``DescribeAutoMLJob`` API.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          exceptions.UnexpectedStatusException: If the auto ml job fails.\n",
      "     |  \n",
      "     |  wait_for_compilation_job(self, job, poll=5)\n",
      "     |      Wait for an Amazon SageMaker Neo compilation job to complete.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          job (str): Name of the compilation job to wait for.\n",
      "     |          poll (int): Polling interval in seconds (default: 5).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          (dict): Return value from the ``DescribeCompilationJob`` API.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          exceptions.UnexpectedStatusException: If the compilation job fails.\n",
      "     |  \n",
      "     |  wait_for_edge_packaging_job(self, job, poll=5)\n",
      "     |      Wait for an Amazon SageMaker Edge packaging job to complete.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          job (str): Name of the edge packaging job to wait for.\n",
      "     |          poll (int): Polling interval in seconds (default: 5).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          (dict): Return value from the ``DescribeEdgePackagingJob`` API.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          exceptions.UnexpectedStatusException: If the compilation job fails.\n",
      "     |  \n",
      "     |  wait_for_endpoint(self, endpoint, poll=30)\n",
      "     |      Wait for an Amazon SageMaker endpoint deployment to complete.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          endpoint (str): Name of the ``Endpoint`` to wait for.\n",
      "     |          poll (int): Polling interval in seconds (default: 5).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict: Return value from the ``DescribeEndpoint`` API.\n",
      "     |  \n",
      "     |  wait_for_job(self, job, poll=5)\n",
      "     |      Wait for an Amazon SageMaker training job to complete.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          job (str): Name of the training job to wait for.\n",
      "     |          poll (int): Polling interval in seconds (default: 5).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          (dict): Return value from the ``DescribeTrainingJob`` API.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          exceptions.UnexpectedStatusException: If the training job fails.\n",
      "     |  \n",
      "     |  wait_for_model_package(self, model_package_name, poll=5)\n",
      "     |      Wait for an Amazon SageMaker endpoint deployment to complete.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          endpoint (str): Name of the ``Endpoint`` to wait for.\n",
      "     |          poll (int): Polling interval in seconds (default: 5).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict: Return value from the ``DescribeEndpoint`` API.\n",
      "     |  \n",
      "     |  wait_for_processing_job(self, job, poll=5)\n",
      "     |      Wait for an Amazon SageMaker Processing job to complete.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          job (str): Name of the processing job to wait for.\n",
      "     |          poll (int): Polling interval in seconds (default: 5).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          (dict): Return value from the ``DescribeProcessingJob`` API.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          exceptions.UnexpectedStatusException: If the compilation job fails.\n",
      "     |  \n",
      "     |  wait_for_transform_job(self, job, poll=5)\n",
      "     |      Wait for an Amazon SageMaker transform job to complete.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          job (str): Name of the transform job to wait for.\n",
      "     |          poll (int): Polling interval in seconds (default: 5).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          (dict): Return value from the ``DescribeTransformJob`` API.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          exceptions.UnexpectedStatusException: If the transform job fails.\n",
      "     |  \n",
      "     |  wait_for_tuning_job(self, job, poll=5)\n",
      "     |      Wait for an Amazon SageMaker hyperparameter tuning job to complete.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          job (str): Name of the tuning job to wait for.\n",
      "     |          poll (int): Polling interval in seconds (default: 5).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          (dict): Return value from the ``DescribeHyperParameterTuningJob`` API.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          exceptions.UnexpectedStatusException: If the hyperparameter tuning job fails.\n",
      "     |  \n",
      "     |  was_processing_job_successful(self, job_name)\n",
      "     |      Calls the DescribeProcessingJob API for the given job name.\n",
      "     |      \n",
      "     |      It returns True if job was successful.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          job_name (str): The name of the processing job to describe.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          bool: Whether the processing job was successful.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  boto_region_name\n",
      "     |      Placeholder docstring\n",
      "\n",
      "FUNCTIONS\n",
      "    container_def(image_uri, model_data_url=None, env=None, container_mode=None)\n",
      "        Create a definition for executing a container as part of a SageMaker model.\n",
      "        \n",
      "        Args:\n",
      "            image_uri (str): Docker image URI to run for this container.\n",
      "            model_data_url (str): S3 URI of data required by this container,\n",
      "                e.g. SageMaker training job model artifacts (default: None).\n",
      "            env (dict[str, str]): Environment variables to set inside the container (default: None).\n",
      "            container_mode (str): The model container mode. Valid modes:\n",
      "                    * MultiModel: Indicates that model container can support hosting multiple models\n",
      "                    * SingleModel: Indicates that model container can support hosting a single model\n",
      "                    This is the default model container mode when container_mode = None\n",
      "        \n",
      "        Returns:\n",
      "            dict[str, str]: A complete container definition object usable with the CreateModel API if\n",
      "            passed via `PrimaryContainers` field.\n",
      "    \n",
      "    get_execution_role(sagemaker_session=None)\n",
      "        Return the role ARN whose credentials are used to call the API.\n",
      "        \n",
      "        Throws an exception if role doesn't exist.\n",
      "        \n",
      "        Args:\n",
      "            sagemaker_session(Session): Current sagemaker session\n",
      "        \n",
      "        Returns:\n",
      "            (str): The role ARN\n",
      "    \n",
      "    pipeline_container_def(models, instance_type=None)\n",
      "        Create a definition for executing a pipeline of containers as part of a SageMaker model.\n",
      "        \n",
      "        Args:\n",
      "            models (list[sagemaker.Model]): this will be a list of ``sagemaker.Model`` objects in the\n",
      "                order the inference should be invoked.\n",
      "            instance_type (str): The EC2 instance type to deploy this Model to. For example,\n",
      "                'ml.p2.xlarge' (default: None).\n",
      "        \n",
      "        Returns:\n",
      "            list[dict[str, str]]: list of container definition objects usable with with the\n",
      "                CreateModel API for inference pipelines if passed via `Containers` field.\n",
      "    \n",
      "    production_variant(model_name, instance_type, initial_instance_count=1, variant_name='AllTraffic', initial_weight=1, accelerator_type=None)\n",
      "        Create a production variant description suitable for use in a ``ProductionVariant`` list.\n",
      "        \n",
      "        This is also part of a ``CreateEndpointConfig`` request.\n",
      "        \n",
      "        Args:\n",
      "            model_name (str): The name of the SageMaker model this production variant references.\n",
      "            instance_type (str): The EC2 instance type for this production variant. For example,\n",
      "                'ml.c4.8xlarge'.\n",
      "            initial_instance_count (int): The initial instance count for this production variant\n",
      "                (default: 1).\n",
      "            variant_name (string): The ``VariantName`` of this production variant\n",
      "                (default: 'AllTraffic').\n",
      "            initial_weight (int): The relative ``InitialVariantWeight`` of this production variant\n",
      "                (default: 1).\n",
      "            accelerator_type (str): Type of Elastic Inference accelerator for this production variant.\n",
      "                For example, 'ml.eia1.medium'.\n",
      "                For more information: https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html\n",
      "        \n",
      "        Returns:\n",
      "            dict[str, str]: An SageMaker ``ProductionVariant`` description\n",
      "    \n",
      "    update_args(args:Dict[str, Any], **kwargs)\n",
      "        Updates the request arguments dict with the value if populated.\n",
      "        \n",
      "        This is to handle the case that the service API doesn't like NoneTypes for argument values.\n",
      "        \n",
      "        Args:\n",
      "            request_args (Dict[str, Any]): the request arguments dict\n",
      "            kwargs: key, value pairs to update the args dict\n",
      "\n",
      "DATA\n",
      "    Any = typing.Any\n",
      "    LOGGER = <Logger sagemaker (WARNING)>\n",
      "    NOTEBOOK_METADATA_FILE = '/opt/ml/metadata/resource-metadata.json'\n",
      "    __warningregistry__ = {'version': 1704, (\"unclosed <ssl.SSLSocket fd=5...\n",
      "    absolute_import = _Feature((2, 5, 0, 'alpha', 1), (3, 0, 0, 'alpha', 0...\n",
      "    print_function = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0)...\n",
      "\n",
      "FILE\n",
      "    /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/sagemaker/session.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sagemaker.session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rasterio\n",
      "  Downloading rasterio-1.2.1-cp36-cp36m-manylinux1_x86_64.whl (19.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 19.1 MB 12.2 MB/s eta 0:00:01     |████████████████████████▋       | 14.7 MB 12.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from rasterio) (2020.12.5)\n",
      "Collecting cligj>=0.5\n",
      "  Downloading cligj-0.7.1-py3-none-any.whl (7.1 kB)\n",
      "Collecting snuggs>=1.4.1\n",
      "  Downloading snuggs-1.4.7-py3-none-any.whl (5.4 kB)\n",
      "Collecting affine\n",
      "  Downloading affine-2.3.0-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from rasterio) (1.18.5)\n",
      "Collecting click-plugins\n",
      "  Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
      "Requirement already satisfied: click<8,>=4.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from rasterio) (7.1.2)\n",
      "Requirement already satisfied: attrs in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from rasterio) (20.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.1.6 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from snuggs>=1.4.1->rasterio) (2.4.7)\n",
      "Installing collected packages: snuggs, cligj, click-plugins, affine, rasterio\n",
      "Successfully installed affine-2.3.0 click-plugins-1.1.1 cligj-0.7.1 rasterio-1.2.1 snuggs-1.4.7\n"
     ]
    }
   ],
   "source": [
    "!pip3 install rasterio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "\n",
    "\n",
    "rasterio_env = rasterio.Env(\n",
    "    #session=aws_session,\n",
    "    GDAL_DISABLE_READDIR_ON_OPEN='NO',\n",
    "    CPL_VSIL_CURL_USE_HEAD='NO',\n",
    "    GDAL_GEOREF_SOURCES='INTERNAL',\n",
    "    GDAL_TIFF_INTERNAL_MASK='NO'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "with rasterio_env as env:\n",
    "    path_to_s3_img = 's3://canopy-production-ml/chips/cloudfree-merge-polygons/split/train/15/15_1400_2800.tif'\n",
    "    with rasterio.open(path_to_s3_img, mode='r', sharing=False, GEOREF_SOURCES='INTERNAL') as src:\n",
    "        train_img = src.read()\n",
    "    # Normalize image\n",
    "train_img = tf.image.convert_image_dtype(train_img, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([18, 100, 100])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio_env as env:\n",
    "    path_to_s3_img = '/vsis3/canopy-production-ml/chips/cloudfree-merge-polygons/split/train/15/15_1400_2800.tif'\n",
    "    with rasterio.open(path_to_s3_img, mode='r', sharing=False, GEOREF_SOURCES='INTERNAL') as src:\n",
    "        train_img = src.read()\n",
    "    # Normalize image\n",
    "train_img = tf.image.convert_image_dtype(train_img, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([18, 100, 100])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Env in module rasterio.env:\n",
      "\n",
      "class Env(builtins.object)\n",
      " |  Abstraction for GDAL and AWS configuration\n",
      " |  \n",
      " |  The GDAL library is stateful: it has a registry of format drivers,\n",
      " |  an error stack, and dozens of configuration options.\n",
      " |  \n",
      " |  Rasterio's approach to working with GDAL is to wrap all the state\n",
      " |  up using a Python context manager (see PEP 343,\n",
      " |  https://www.python.org/dev/peps/pep-0343/). When the context is\n",
      " |  entered GDAL drivers are registered, error handlers are\n",
      " |  configured, and configuration options are set. When the context\n",
      " |  is exited, drivers are removed from the registry and other\n",
      " |  configurations are removed.\n",
      " |  \n",
      " |  Example\n",
      " |  -------\n",
      " |  .. code-block:: python\n",
      " |  \n",
      " |      with rasterio.Env(GDAL_CACHEMAX=128000000) as env:\n",
      " |          # All drivers are registered, GDAL's raster block cache\n",
      " |          # size is set to 128 MB.\n",
      " |          # Commence processing...\n",
      " |          ...\n",
      " |          # End of processing.\n",
      " |  \n",
      " |      # At this point, configuration options are set to their\n",
      " |      # previous (possible unset) values.\n",
      " |  \n",
      " |  A boto3 session or boto3 session constructor arguments\n",
      " |  `aws_access_key_id`, `aws_secret_access_key`, `aws_session_token`\n",
      " |  may be passed to Env's constructor. In the latter case, a session\n",
      " |  will be created as soon as needed. AWS credentials are configured\n",
      " |  for GDAL as needed.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __enter__(self)\n",
      " |  \n",
      " |  __exit__(self, exc_type=None, exc_val=None, exc_tb=None)\n",
      " |  \n",
      " |  __init__(self, session=None, aws_unsigned=False, profile_name=None, session_class=<function Session.aws_or_dummy at 0x7fdf82342f28>, **options)\n",
      " |      Create a new GDAL/AWS environment.\n",
      " |      \n",
      " |      Note: this class is a context manager. GDAL isn't configured\n",
      " |      until the context is entered via `with rasterio.Env():`\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      session : optional\n",
      " |          A Session object.\n",
      " |      aws_unsigned : bool, optional\n",
      " |          Do not sign cloud requests.\n",
      " |      profile_name : str, optional\n",
      " |          A shared credentials profile name, as per boto3.\n",
      " |      session_class : Session, optional\n",
      " |          A sub-class of Session.\n",
      " |      **options : optional\n",
      " |          A mapping of GDAL configuration options, e.g.,\n",
      " |          `CPL_DEBUG=True, CHECK_WITH_INVERT_PROJ=False`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Env\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      We raise EnvError if the GDAL config options\n",
      " |      AWS_ACCESS_KEY_ID or AWS_SECRET_ACCESS_KEY are given. AWS\n",
      " |      credentials are handled exclusively by boto3.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      \n",
      " |      >>> with Env(CPL_DEBUG=True, CPL_CURL_VERBOSE=True):\n",
      " |      ...     with rasterio.open(\"https://example.com/a.tif\") as src:\n",
      " |      ...         print(src.profile)\n",
      " |      \n",
      " |      For access to secured cloud resources, a Rasterio Session or a\n",
      " |      foreign session object may be passed to the constructor.\n",
      " |      \n",
      " |      >>> import boto3\n",
      " |      >>> from rasterio.session import AWSSession\n",
      " |      >>> boto3_session = boto3.Session(...)\n",
      " |      >>> with Env(AWSSession(boto3_session)):\n",
      " |      ...     with rasterio.open(\"s3://mybucket/a.tif\") as src:\n",
      " |      ...         print(src.profile)\n",
      " |  \n",
      " |  credentialize(self)\n",
      " |      Get credentials and configure GDAL\n",
      " |      \n",
      " |      Note well: this method is a no-op if the GDAL environment\n",
      " |      already has credentials, unless session is not None.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      None\n",
      " |  \n",
      " |  drivers(self)\n",
      " |      Return a mapping of registered drivers.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  default_options() from builtins.type\n",
      " |      Default configuration options\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      None\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict\n",
      " |  \n",
      " |  from_defaults(*args, **kwargs) from builtins.type\n",
      " |      Create an environment with default config options\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      args : optional\n",
      " |          Positional arguments for Env()\n",
      " |      kwargs : optional\n",
      " |          Keyword arguments for Env()\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Env\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The items in kwargs will be overlaid on the default values.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(rasterio.Env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow2_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
