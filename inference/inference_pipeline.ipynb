{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Data Mount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mount: exec /Library/Filesystems/nfs4.fs/Contents/Resources/mount_nfs4 for /Users/purgatorid/Documents/GitHub/Project Canopy/cb_feature_detection/inference/efs_inference_data: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!mount -t nfs4 -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport 172.31.91.151:/ ./efs_inference_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Docker Run / Sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rasterio geopandas shapely tensorflow-addons[tensorflow]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Local / Sagemaker Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rasterio as rio\n",
    "import numpy as np\n",
    "from rasterio.windows import Window\n",
    "from glob import glob\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.geometry import box\n",
    "import geopandas as gpd\n",
    "from rasterio.windows import get_data_window\n",
    "import rasterio as rio\n",
    "from inference_predict import *\n",
    "import boto3\n",
    "import matplotlib.pyplot as plt\n",
    "import gdal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Windowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_windows(img_dim, patch_size=(240, 240), stride=(240, 240)):\n",
    "    patch_size = np.array(patch_size)\n",
    "    stride = np.array(stride)\n",
    "    img_dim = np.array(img_dim)\n",
    "    # to take into account edges, add additional blocks around right side edge and bottom edge of raster\n",
    "    new_img_dim = [img_dim[0] + stride[0],img_dim[1] + stride[0]]\n",
    "    \n",
    "    max_dim = (new_img_dim//patch_size)*patch_size - patch_size\n",
    "\n",
    "    ys = np.arange(0, img_dim[0], stride[0])\n",
    "    xs = np.arange(0, img_dim[1], stride[1])\n",
    "\n",
    "    tlc = np.array(np.meshgrid(ys, xs)).T.reshape(-1, 2)\n",
    "    tlc = tlc[tlc[:, 0] <= max_dim[0]]\n",
    "    tlc = tlc[tlc[:, 1] <= max_dim[1]]\n",
    "    \n",
    "    windows = []\n",
    "    for y,x in tlc.astype(int):\n",
    "        windows.append(Window(x, y, patch_size[1], patch_size[0]))\n",
    "\n",
    "    return windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ndvi(data, dtype_1=rio.float32):\n",
    "    \n",
    "    nir = data[3].astype(dtype_1)\n",
    "    red = data[2].astype(dtype_1)\n",
    "\n",
    "    # Allow division by zero\n",
    "    np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "    # Calculate NDVI\n",
    "    ndvi = ((nir - red) / (nir + red)).astype(dtype_1)\n",
    "\n",
    "    # Rescaling for use in 16bit output\n",
    "\n",
    "    ndvi = (ndvi + 1) * (2**15 - 1)\n",
    "\n",
    "    # Add NDVI band to end of array    \n",
    "    rast = np.concatenate((data,[ndvi]),axis=0)\n",
    "    \n",
    "    rast = rast.astype(rio.uint16)\n",
    "    \n",
    "    return rast\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Model Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_url = \"s3://canopy-production-ml/inference/model_files/model-best.h5\"\n",
    "weights_url = \"s3://canopy-production-ml/inference/model_files/model_weights_best.h5\"\n",
    "\n",
    "download_model(model_url,weights_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"model.h5\",\"model_weights.h5\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = [\"Industrial_agriculture\",\"ISL\",\"Mining\",\"Roads\",\"Shifting_cultivation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_windows(granule_dir,patch_size=100,\n",
    "                   stride=100,SAVE=False,SAVE_INDIVIDUAL=False,\n",
    "                   bands=[2, 3, 4, 8, 11, 12], \n",
    "                  model=model,\n",
    "                   predict_thresh=.5,\n",
    "                  label_list=label_list, \n",
    "                  job_name=\"test_inference\", \n",
    "                  output_filename=\"./inference_output/result.json\"):\n",
    "    \n",
    "    granule_list = glob(f'{granule_dir}/*.tif')\n",
    "    \n",
    "    output_dict = {}\n",
    "    \n",
    "    granule_id_list = []\n",
    "    \n",
    "    window_id_list = []\n",
    "    \n",
    "    window_geom_list = []\n",
    "    \n",
    "    data_list = []\n",
    "    \n",
    "    label_master_list = []\n",
    "    \n",
    "    gdf_list = []\n",
    "    \n",
    "    timestamp = gen_timestamp()\n",
    "    \n",
    "    for j,granule_path in enumerate(granule_list):\n",
    "        \n",
    "        granule_id = granule_path.split(\"/\")[-1].split(\"_\")[0]\n",
    "    \n",
    "        with rio.open(granule_path) as src:\n",
    "\n",
    "            windows = get_windows(src.shape, (patch_size, patch_size), (stride, stride))\n",
    "\n",
    "            for i, window in enumerate(windows):\n",
    "                \n",
    "                print(f\"predicting window {i + 1} of {len(windows)} of granulate {j + 1} of {len(granule_list)}\",end='\\r', flush=True)\n",
    "                \n",
    "                label_name_list = []\n",
    "                \n",
    "                window_id = i+1\n",
    "\n",
    "                data = src.read(bands,window=window, masked=True)\n",
    "\n",
    "                data = add_ndvi(data)\n",
    "        \n",
    "                shape = data.shape\n",
    "            \n",
    "                new_shape = (data.shape[0],patch_size,patch_size)\n",
    "            \n",
    "                if shape != new_shape:\n",
    "\n",
    "                    filled_array = np.full(new_shape, 0)\n",
    "                    filled_array[:shape[0],:shape[1],:shape[2]] = data\n",
    "                    data = filled_array\n",
    "                    window = Window(window.col_off,window.row_off,shape[2],shape[1])\n",
    "                    \n",
    "                    \n",
    "                #image pre-processing / inference\n",
    "                prediction = model.predict(read_image_tf_out(data))\n",
    "                prediction = np.where(prediction > predict_thresh, 1, 0)\n",
    "                prediction_i = np.where(prediction == 1)[1]\n",
    "                for i in prediction_i:\n",
    "                    label_name_list.append(label_list[i])\n",
    "                \n",
    "                label_master_list.append(label_name_list)\n",
    "                \n",
    "                #vectorizing raster bounds for visualization \n",
    "                window_bounds = rio.windows.bounds(window, src.transform, height=patch_size, width=patch_size)\n",
    "                geom = box(*window_bounds)\n",
    "                geom_coords = list(geom.exterior.coords)\n",
    "#                 window_geom_list.append(geom)\n",
    "                \n",
    "                #create or append to dict....\n",
    "                \n",
    "                if granule_id in output_dict:\n",
    "\n",
    "                    output_dict[granule_id].append({\"window_id\":window_id,\"polygon_coords\":geom_coords,\"labels\":label_name_list})\n",
    "\n",
    "                else:\n",
    "\n",
    "                    output_dict[granule_id] = [{\"window_id\":window_id,\"polygon_coords\":geom_coords,\"labels\":label_name_list}]\n",
    "        \n",
    "        save_to_s3(output_dict,output_filename,job_name,timestamp)\n",
    "        \n",
    "\n",
    "\n",
    "#             gdf = gpd.GeoDataFrame({\"granule_id\":granule_id_list,\"window_id\":window_id_list,\"geometry\":window_geom_list,\"labels\":label_master_list})\n",
    "#             gdf[\"labels\"] = gdf[\"labels\"].astype(str)\n",
    "\n",
    "#             gdf_list.append(gdf)\n",
    "            \n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting window 1936 of 1936 of granulate 1 of 1\r"
     ]
    }
   ],
   "source": [
    "# granule_dir = \"./efs_inference_data/\"\n",
    "granule_dir = \"/Users/purgatorid/Downloads/granule_test/\"\n",
    "\n",
    "output_dict = output_windows(granule_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = output_dict\n",
    "\n",
    "count = {}\n",
    "label_match_results = []\n",
    "granule_count = len(data.keys())\n",
    "granule_list = data.keys()\n",
    "count[\"granule_count\"] = granule_count\n",
    "for k1 in list(data.keys()):\n",
    "    for i in range(len(data[k1])):\n",
    "        if len(data[k1][i]['labels']) == 0:\n",
    "            if \"null_chips\" not in count.keys():\n",
    "                count[\"null_chips\"] = 1\n",
    "            else:\n",
    "                count[\"null_chips\"] += 1 \n",
    "        for label in data[k1][i]['labels']:\n",
    "            if label not in count.keys():\n",
    "                count[label] = 1 \n",
    "            else:\n",
    "                    count[label] += 1 \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'granule_count': 1,\n",
       " 'null_chips': 1512,\n",
       " 'Shifting_cultivation': 336,\n",
       " 'ISL': 82,\n",
       " 'Roads': 5,\n",
       " 'Industrial_agriculture': 1}"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (len(output_dict['101'])):\n",
    "    print(output_dict['101'][i]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_gdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_gdf.to_file(\"./inference_output/test.geojson\", driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.to_file(\"./inference_output/test.geojson\", driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Output Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_output_files(json_path,download=True, filepath = \"predict_test-2021-05-03-23-37-03.json\", label_match=\"ISL\"):\n",
    "\n",
    "    s3 = boto3.resource('s3')\n",
    "\n",
    "    #Download Model, Weights\n",
    "    \n",
    "    if download:\n",
    "        \n",
    "        bucket = json_path.split(\"/\")[2]\n",
    "        model_key = \"/\".join(json_path.split(\"/\")[3:])\n",
    "        filename = json_path.split(\"/\")[-1]\n",
    "        s3.Bucket(bucket).download_file(model_key, filename )\n",
    "        filepath = filename\n",
    "    \n",
    "    with open(filepath) as jsonfile:\n",
    "        data = json.load(jsonfile)\n",
    "        \n",
    "\n",
    "    count = {}\n",
    "    label_match_results = []\n",
    "    granule_count = len(data.keys())\n",
    "    granule_list = data.keys()\n",
    "    count[\"granule_count\"] = granule_count\n",
    "    for k1 in list(data.keys()):\n",
    "        for i in range(len(data[k1])):\n",
    "            if len(data[k1][i]['labels']) == 0:\n",
    "                if \"null_chips\" not in count.keys():\n",
    "                    count[\"null_chips\"] = 1\n",
    "                else:\n",
    "                    count[\"null_chips\"] += 1 \n",
    "            for label in data[k1][i]['labels']:\n",
    "                if label == label_match:\n",
    "                    label_match_results.append([k1,data[k1][i]])\n",
    "                if label not in count.keys():\n",
    "                    count[label] = 1 \n",
    "                else:\n",
    "                    count[label] += 1 \n",
    "    return count, label_match_results, granule_list, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = \"s3://canopy-production-ml/inference/output/inference_output_test-2021-05-07-19-29-53.json\"\n",
    "\n",
    "count, match_results, granule_list, data = process_output_files(json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygon_list = []\n",
    "for result in match_results:\n",
    "    coords = result[1][\"polygon_coords\"]\n",
    "    polygon = Polygon(coords)\n",
    "    polygon_list.append(polygon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.GeoDataFrame({\"geometry\":polygon_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gdf.set_crs(epsg=3257)\n",
    "        \n",
    "gdf = gdf.to_crs(epsg=4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.to_file(\"./inference_output/test_ISL.geojson\", driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-11169700.787427548, -4193553.7119406024],\n",
       " [-11169700.787427548, -4191764.5839900365],\n",
       " [-11171489.915378114, -4191764.5839900365],\n",
       " [-11171489.915378114, -4193553.7119406024],\n",
       " [-11169700.787427548, -4193553.7119406024]]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['test.tif'][0][\"polygon_coords\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Vectorized Predicted Granules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s3_dir_match(s3_dir_url,granule_list):\n",
    "    \n",
    "\n",
    "    objs = []\n",
    "    bucket = s3_dir_url.split(\"/\")[2]\n",
    "    key = \"/\".join(s3_dir_url.split(\"/\")[3:5])\n",
    "\n",
    "    s3 = boto3.resource('s3')\n",
    "    my_bucket = s3.Bucket(bucket)\n",
    "\n",
    "    window_geom_list = []\n",
    "    granule_id_list = []\n",
    "    for obj in my_bucket.objects.filter(Prefix=key):\n",
    "        granule_id = obj.key.split(\"/\")[-1].split(\"_\")[0]\n",
    "        if granule_id in granule_list:\n",
    "            obj_url = \"s3://\" + bucket + \"/\" + obj.key\n",
    "            with rio.open(obj_url) as src:\n",
    "                bounds = src.bounds\n",
    "                geom = box(*bounds)\n",
    "                window_geom_list.append(geom)\n",
    "                granule_id_list.append(granule_id)\n",
    "    gdf = gpd.GeoDataFrame({\"geometry\":window_geom_list,\"granule_id\":granule_id_list})\n",
    "                \n",
    "\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = s3_dir_match(\"s3://canopy-production-ml/full_congo_basin/02.17.21_CB_GEE_Pull/\",granule_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.to_file(\"granules.json\", driver=\"GeoJSON\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and Export GDF of Original Labels Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"/Users/purgatorid/Downloads/polygons_021521.csv\"\n",
    "\n",
    "\n",
    "df = pd.read_csv(\n",
    "    FILE_NAME)\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    df,\n",
    "    crs={'init': 'epsg:4326'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons = []\n",
    "for polygon in df[\"polygon\"]:\n",
    "    polygons.append(Polygon(json.loads(polygon)[\"coordinates\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf[\"geometry\"] = polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.loc[90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.to_file(\"output.json\", driver=\"GeoJSON\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Reproject One Granulate Containing ISL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_raster(input_file, dest_dir, epsg_format='EPSG:3257', windows=False):\n",
    "    \"\"\"Converts the rasters in the src_dir into a different EPSG format,\n",
    "    keeping the same folder structure and saving them in the dest_dir.\"\"\"\n",
    "    \n",
    "    print(input_file)\n",
    "\n",
    "    filename = \"test.tif\"\n",
    "#     print(filename)\n",
    "\n",
    "    # If the respective grouping folders are not available \n",
    "\n",
    "    output_filepath = dest_dir + filename\n",
    "    print(output_filepath)\n",
    "\n",
    "\n",
    "#         Finally, we convert\n",
    "    converted = gdal.Warp(output_filepath, [input_file],format='GTiff',\n",
    "                          dstSRS=epsg_format, resampleAlg='near')\n",
    "    converted = None\n",
    "        \n",
    "    print('Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "granule = \"/Users/purgatorid/Downloads/1241_full_congo_export_v12_all_bands_Feb_11_12_44_53_2021.tif\"\n",
    "dest_dir = \"/Users/purgatorid/Downloads/\"\n",
    "\n",
    "convert_raster(granule,dest_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Results (Incomplete Code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(match_results,s3_url):\n",
    "    for window in match_results:\n",
    "        granule_id = window[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = {1,2,4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Without Windows Code - Direct Chip Predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_predictions(granule_dir,patch_size=100,\n",
    "                   stride=100,SAVE=False,SAVE_INDIVIDUAL=False,\n",
    "                   bands=[2, 3, 4, 8, 11, 12], \n",
    "                  model=model,\n",
    "                   predict_thresh=.5,\n",
    "                  label_list=label_list, \n",
    "                  job_name=\"test_inference\", \n",
    "                  output_filename=\"./inference_output/result.json\", \n",
    "                      apply_windows=False, \n",
    "                      read_process=\"read_img_tf_out\", \n",
    "                      sample_frac=1):\n",
    "    \n",
    "    granule_list = glob(f'{granule_dir}/*.tif')\n",
    "    \n",
    "    end = len(granule_list) // sample_frac \n",
    "    \n",
    "    granule_list = granule_list[0:end]\n",
    "    \n",
    "    print(f\"running inference on {len(granule_list)} chips\")\n",
    "    \n",
    "    output_dict = {}\n",
    "    \n",
    "    granule_id_list = []\n",
    "    \n",
    "    window_id_list = []\n",
    "    \n",
    "    window_geom_list = []\n",
    "    \n",
    "    data_list = []\n",
    "    \n",
    "    label_master_list = []\n",
    "    \n",
    "    gdf_list = []\n",
    "    \n",
    "    timestamp = gen_timestamp()\n",
    "    \n",
    "    missed_chips = []\n",
    "    \n",
    "    for j,granule_path in enumerate(granule_list):\n",
    "        \n",
    "        print(f'running inference on {j} of {len(granule_list)}',end='\\r', flush=True)\n",
    "        \n",
    "        label_name_list = []\n",
    "        \n",
    "        granule_id = granule_path.split(\"/\")[-1].split(\"_\")[0]\n",
    "        filepath = granule_path.split(\"/\")[-1]\n",
    "        \n",
    "        if filepath:\n",
    "\n",
    "            with rio.open(granule_path) as src:\n",
    "\n",
    "                data = src.read(bands,masked=True)\n",
    "\n",
    "                data = add_ndvi(data)\n",
    "\n",
    "                shape = data.shape\n",
    "\n",
    "                if apply_windows:\n",
    "\n",
    "                    new_shape = (data.shape[0],patch_size,patch_size)\n",
    "\n",
    "                    if shape != new_shape:\n",
    "\n",
    "                        filled_array = np.full(new_shape, 0)\n",
    "                        filled_array[:shape[0],:shape[1],:shape[2]] = data\n",
    "                        data = filled_array\n",
    "                        window = Window(window.col_off,window.row_off,shape[2],shape[1])\n",
    "\n",
    "                #image pre-processing / inference\n",
    "\n",
    "\n",
    "                if read_process == \"read_img_tf_out\":\n",
    "                    read_func = read_image_tf_out\n",
    "                else:\n",
    "                    read_func = read_image\n",
    "\n",
    "                prediction = model.predict(read_func(data))\n",
    "#                 print(\"original_prediction:\",prediction)\n",
    "                prediction = np.where(prediction > predict_thresh, 1, 0)\n",
    "#                 print(\"sigmoid prediction gate:\",prediction)\n",
    "                prediction_i = np.where(prediction == 1)[1]\n",
    "                if 1 not in np.where(prediction == 1)[1]:\n",
    "                    missed_chips.append(granule_path)\n",
    "#                 print(\"index of matching labels:\",prediction_i)\n",
    "                for i in prediction_i:\n",
    "                    label_name_list.append(label_list[i])\n",
    "\n",
    "                label_master_list.append(label_name_list)\n",
    "\n",
    "                #vectorizing raster bounds for visualization \n",
    "                data_bounds = src.bounds\n",
    "                geom = box(*data_bounds)\n",
    "                geom_coords = list(geom.exterior.coords)\n",
    "    #                 window_geom_list.append(geom)\n",
    "\n",
    "                #create or append to dict....\n",
    "\n",
    "                if granule_id in output_dict:\n",
    "\n",
    "                    output_dict[granule_id].append({\"polygon_coords\":geom_coords,\"labels\":label_name_list})\n",
    "\n",
    "                else:\n",
    "\n",
    "                    output_dict[granule_id] = [{\"polygon_coords\":geom_coords,\"labels\":label_name_list}]\n",
    "\n",
    "        save_to_s3(output_dict,output_filename,job_name,timestamp)\n",
    "\n",
    "\n",
    "\n",
    "    #             gdf = gpd.GeoDataFrame({\"granule_id\":granule_id_list,\"window_id\":window_id_list,\"geometry\":window_geom_list,\"labels\":label_master_list})\n",
    "    #             gdf[\"labels\"] = gdf[\"labels\"].astype(str)\n",
    "\n",
    "    #             gdf_list.append(gdf)\n",
    "\n",
    "    return output_dict,missed_chips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running inference on 75 chips\n",
      "running inference on 74 of 75\r"
     ]
    }
   ],
   "source": [
    "# granule_dir = \"./efs_inference_data/\"\n",
    "granule_dir_local = \"/Volumes/Lacie/zhenyadata/Project_Canopy_Data/PC_Data/Sentinel_Data/Chips/misha_polygons_cloudfreemerge/yes/ISL/100/91/\"\n",
    "# granule_dir_efs = \n",
    "\n",
    "output_dict,missed_chips = output_predictions(granule_dir_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"file_path\":missed_chips})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"missed_chips.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = output_dict\n",
    "\n",
    "count = {}\n",
    "label_match_results = []\n",
    "granule_count = len(data.keys())\n",
    "granule_list = data.keys()\n",
    "count[\"granule_count\"] = granule_count\n",
    "for k1 in list(data.keys()):\n",
    "    for i in range(len(data[k1])):\n",
    "        if len(data[k1][i]['labels']) == 0:\n",
    "            if \"null_chips\" not in count.keys():\n",
    "                count[\"null_chips\"] = 1\n",
    "            else:\n",
    "                count[\"null_chips\"] += 1 \n",
    "        for label in data[k1][i]['labels']:\n",
    "            if label not in count.keys():\n",
    "                count[label] = 1 \n",
    "            else:\n",
    "                    count[label] += 1 \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'granule_count': 1,\n",
       " 'ISL': 43,\n",
       " 'Shifting_cultivation': 24,\n",
       " 'Roads': 3,\n",
       " 'null_chips': 3,\n",
       " 'Industrial_agriculture': 2}"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histogram for Numpy Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_hist(arr,tensor=True):\n",
    "    \n",
    "    if tensor:\n",
    "        arr = np.array(arr)\n",
    "        arr = np.transpose(arr[0], (2, 1, 0))\n",
    "\n",
    "\n",
    "    for i in range(arr.shape[0]):\n",
    "        band_np = arr[i].flatten()\n",
    "        plt.hist(band_np,label=str(i))\n",
    "\n",
    "\n",
    "    plt.legend(prop={'size': 10})\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_hist(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_hist(data1,tensor=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "infer-conda",
   "language": "python",
   "name": "infer-conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
